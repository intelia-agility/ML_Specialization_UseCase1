{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c01d42d4-08ef-4770-b1d2-a0a834fc4b33",
   "metadata": {},
   "source": [
    "<h2 style='color:darkblue; text-align:center;'><strong>Taxi Demand Forecasting on Vertex AI </strong></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6169396-26d7-4535-ad78-40c7495306a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 02:58:38.568168: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-13 02:58:39.979269: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6Status12empty_stringB5cxx11Ev']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZNK10tensorflow4data11DatasetBase8FinalizeEPNS_15OpKernelContextESt8functionIFN3tsl8StatusOrISt10unique_ptrIS1_NS5_4core15RefCountDeleterEEEEvEE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n",
      "WARNING:root:TensorFlow Decision Forests 1.5.0 is compatible with the following TensorFlow Versions: ['2.13.0']. However, TensorFlow 2.13.1 was detected. This can cause issues with the TF API and symbols in the custom C++ ops. See the TF and TF-DF compatibility table at https://github.com/tensorflow/decision-forests/blob/main/documentation/known_issues.md#compatibility-table.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import shutil\n",
    "import tempfile\n",
    "import urllib.request\n",
    "from absl import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_data_validation as tfdv\n",
    "\n",
    "import tensorflow_model_analysis as tfma\n",
    "from tfx import v1 as tfx\n",
    "\n",
    "# Setting up logging and pprint\n",
    "# tf.get_logger().propagate = False\n",
    "pp = pprint.PrettyPrinter()\n",
    "# Set TensorFlow logging to error-only\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0 = all, 1 = no info, 2 = no info and warnings, 3 = no info, warnings, and errors\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3234dcd3-4797-4573-b4a1-99ee6df515eb",
   "metadata": {},
   "source": [
    "***Paths***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41464264-93f3-4277-a676-5fbe60a02035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_ROOT: gs://chicago_taxitrips/pipeline_root/taxi-demand-prediction-management\n",
      "DATA_DIRECTORY: gs://chicago_taxitrips/DATA_DIRECTORY\n",
      "TRAIN_DATA_PATH: gs://chicago_taxitrips/DATA_DIRECTORY/training_data.csv\n",
      "TEST_DATA_PATH: gs://chicago_taxitrips/DATA_DIRECTORY/test_data.csv\n",
      "EVAL_DATA_PATH: gs://chicago_taxitrips/DATA_DIRECTORY/validation_data.csv\n",
      "PIPELINE_ROOT: gs://chicago_taxitrips/pipeline_root/taxi-demand-prediction-management\n",
      "SERVING_MODEL_DIR: gs://chicago_taxitrips/serving_model/taxi-demand-prediction-management\n"
     ]
    }
   ],
   "source": [
    "GOOGLE_CLOUD_PROJECT = 'mlops-363723'         \n",
    "GOOGLE_CLOUD_PROJECT_NUMBER = '75674212269'  \n",
    "GOOGLE_CLOUD_REGION = 'us-central1'          \n",
    "GCS_BUCKET_NAME = 'chicago_taxitrips'  \n",
    "\n",
    "PIPELINE_NAME = 'taxi-demand-prediction-management'\n",
    "# Define the GCS_BUCKET_NAME\n",
    "GCS_BUCKET_NAME = 'chicago_taxitrips'\n",
    "DATA_DIRECTORY = 'gs://chicago_taxitrips/DATA_DIRECTORY'\n",
    "\n",
    "# # Paths for users' data.\n",
    "TRAIN_DATA_FILENAME = 'training_data.csv'\n",
    "EVAL_DATA_FILENAME = 'validation_data.csv'\n",
    "TEST_DATA_FILENAME = 'test_data.csv'\n",
    "\n",
    "TRAIN_DATA_PATH = os.path.join(DATA_DIRECTORY, TRAIN_DATA_FILENAME)\n",
    "EVAL_DATA_PATH = os.path.join(DATA_DIRECTORY, EVAL_DATA_FILENAME)\n",
    "TEST_DATA_PATH = os.path.join(DATA_DIRECTORY, TEST_DATA_FILENAME)\n",
    "\n",
    "\n",
    "# Path to various pipeline artifact.\n",
    "PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(\n",
    "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "# Paths for users' Python module.\n",
    "MODULE_ROOT = 'gs://{}/pipeline_module/{}'.format(\n",
    "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "\n",
    "SERVING_MODEL_DIR = 'gs://{}/serving_model/{}'.format(\n",
    "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "# Name of Vertex AI Endpoint.\n",
    "ENDPOINT_NAME = 'prediction-' + PIPELINE_NAME\n",
    "\n",
    "print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))\n",
    "print('DATA_DIRECTORY:', DATA_DIRECTORY)\n",
    "print('TRAIN_DATA_PATH:', TRAIN_DATA_PATH)\n",
    "print('TEST_DATA_PATH:', TEST_DATA_PATH)\n",
    "print('EVAL_DATA_PATH:', EVAL_DATA_PATH)\n",
    "print('PIPELINE_ROOT:', PIPELINE_ROOT)\n",
    "print('SERVING_MODEL_DIR:', SERVING_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e6781dc-9c72-4ed6-836a-43a249cbbb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "_taxi_constants_module_file = 'taxi_constants.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b999b74a-8a83-495e-96f6-e2135e06b1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting taxi_constants.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_taxi_constants_module_file}\n",
    "NUMERICAL_FEATURES = [ \n",
    "    'temperature_2m',\n",
    "    'relativehumidity_2m',\n",
    "    'rain',\n",
    "    'snowfall',\n",
    "    'hour_sin',\n",
    "    'hour_cos',\n",
    "    'day_sin',\n",
    "    'day_cos',\n",
    "    'month_sin',\n",
    "    'month_cos']\n",
    "\n",
    "\n",
    "\n",
    "CATEGORICAL_NUMERICAL_FEATURES = [\n",
    "    'public_holiday', 'weathercode','pickup_community_area', 'year'\n",
    "]\n",
    "\n",
    "\n",
    "# Keys\n",
    "LABEL_KEY = 'demand'\n",
    "\n",
    "def t_name(key):\n",
    "    \"\"\"\n",
    "    Rename the feature keys so that they don't clash with the raw keys when\n",
    "    running the Evaluator component.\n",
    "    Args:\n",
    "    key: The original feature key\n",
    "    Returns:\n",
    "    key with '_xf' appended\n",
    "    \"\"\"\n",
    "    return key + '_xf'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d7489d9-6b3a-45b4-b3a1-d6c2113ee07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORM_MODULE_PATH = \"chicago_taxi_transform.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3330792-7b00-43b3-a4e5-79336d25c18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting chicago_taxi_transform.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRANSFORM_MODULE_PATH}\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import taxi_constants\n",
    "\n",
    "_NUMERICAL_FEATURES = taxi_constants.NUMERICAL_FEATURES\n",
    "_CATEGORICAL_NUMERICAL_FEATURES = taxi_constants.CATEGORICAL_NUMERICAL_FEATURES\n",
    "_LABEL_KEY = taxi_constants.LABEL_KEY\n",
    "\n",
    "def _fill_in_missing(x):\n",
    "    \"\"\"Replace missing values in a SparseTensor.\"\"\"\n",
    "    default_value = '' if x.dtype == tf.string else 0\n",
    "    if not isinstance(x, tf.sparse.SparseTensor):\n",
    "        return x\n",
    "    return tf.squeeze(\n",
    "        tf.sparse.to_dense(\n",
    "            tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n",
    "            default_value),\n",
    "        axis=1)\n",
    "\n",
    "def z_score_normalization(inputs, feature_name):\n",
    "    \"\"\"Apply Z-score normalization on a feature.\"\"\"\n",
    "    return tft.scale_to_z_score(_fill_in_missing(inputs[feature_name]))\n",
    "\n",
    "\n",
    "def log_transformation(inputs, feature_name):\n",
    "    \"\"\"Apply log transformation on a feature.\"\"\"\n",
    "    # Ensure the input feature is in float32\n",
    "    feature_values = tf.cast(inputs[feature_name], tf.float32)\n",
    "\n",
    "    # Replace non-positive values with a small positive number\n",
    "    feature_values_safe = tf.where(\n",
    "        feature_values <= 0, \n",
    "        tf.constant(0.01, dtype=tf.float32),\n",
    "        feature_values\n",
    "    )\n",
    "\n",
    "    return tf.math.log1p(feature_values_safe)\n",
    "\n",
    "def square_root_transformation(inputs, feature_name):\n",
    "    \"\"\"Apply square root transformation on a feature.\"\"\"\n",
    "    feature_values = tf.where(inputs[feature_name] <= 0, tf.constant(0.01, dtype=tf.float32), inputs[feature_name])\n",
    "    return tf.math.sqrt(feature_values)\n",
    "\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"Preprocess input columns into transformed columns.\"\"\"\n",
    "    outputs = {}\n",
    "    \n",
    "    # Numerical Features: Apply Z-score normalization.\n",
    "    for key in _NUMERICAL_FEATURES:\n",
    "        outputs[taxi_constants.t_name(key)] = z_score_normalization(inputs, key)\n",
    "    \n",
    "    # Categorical Numerical Features\n",
    "    for key in _CATEGORICAL_NUMERICAL_FEATURES:\n",
    "        # Ensure the feature exists in the inputs\n",
    "        if key in inputs:\n",
    "            outputs[taxi_constants.t_name(key)] = _fill_in_missing(inputs[key])\n",
    "\n",
    "    # Feature Transformations\n",
    "    outputs['log_trip_total'] = log_transformation(inputs, 'trip_total')\n",
    "    outputs['log_trip_miles'] = log_transformation(inputs, 'trip_miles')\n",
    "    outputs['log_duration'] = log_transformation(inputs, 'duration')\n",
    "    outputs['sqrt_precipitation'] = square_root_transformation(inputs, 'precipitation')\n",
    "    \n",
    "    if _LABEL_KEY in inputs:\n",
    "        \n",
    "        outputs[_LABEL_KEY]= log_transformation(inputs, _LABEL_KEY)\n",
    "        \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76572a50-eda6-4e4b-a8d3-4d1071018be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_taxi_trainer_module_file = 'taxi_trainer.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48e26d2a-b338-4982-855e-83a3a45b80bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting taxi_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_taxi_trainer_module_file}\n",
    "\n",
    "from typing import NamedTuple, Dict, List, Text, Any\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tfx import v1 as tfx\n",
    "from keras_tuner.engine import base_tuner\n",
    "from keras_tuner import HyperParameters, RandomSearch\n",
    "from tfx_bsl.public import tfxio\n",
    "from tfx.components.trainer.fn_args_utils import FnArgs\n",
    "import os\n",
    "import taxi_constants\n",
    "\n",
    "_LABEL_KEY = taxi_constants.LABEL_KEY\n",
    "\n",
    "\n",
    "_BATCH_SIZE = 64\n",
    "\n",
    "TunerFnResult = NamedTuple('TunerFnResult', [('tuner', RandomSearch), ('fit_kwargs', Dict[Text, Any])])  # Changed to Randomsearch\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_mean_absolute_error',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "def _input_fn(file_pattern: List[Text],\n",
    "              data_accessor: tfx.components.DataAccessor,\n",
    "              tf_transform_output: tft.TFTransformOutput,\n",
    "              batch_size: int = _BATCH_SIZE) -> tf.data.Dataset:\n",
    "    return data_accessor.tf_dataset_factory(\n",
    "        file_pattern,\n",
    "        tfxio.TensorFlowDatasetOptions(batch_size=batch_size, label_key=_LABEL_KEY),\n",
    "        tf_transform_output.transformed_metadata.schema).repeat()\n",
    "\n",
    "\n",
    "def _build_keras_model(hp, tf_transform_output: tft.TFTransformOutput) -> tf.keras.Model:\n",
    "    # Define feature specs and create input layers\n",
    "    feature_spec = tf_transform_output.transformed_feature_spec().copy()\n",
    "    # Remove the label feature\n",
    "    feature_spec.pop(_LABEL_KEY)\n",
    "    inputs = {\n",
    "        key: tf.keras.layers.Input(shape=(1,), name=key)\n",
    "        for key in feature_spec.keys()\n",
    "    }\n",
    "\n",
    "    # Concatenate all input features\n",
    "    concatenated_inputs = tf.keras.layers.Concatenate()(list(inputs.values()))\n",
    "\n",
    "    num_layers = hp.Int('num_layers', 1, 5)\n",
    "    activation_choice = hp.Choice('activation', ['relu', 'leaky_relu', 'elu', 'tanh', 'sigmoid'])\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        units = hp.Int(f'units_{i}', min_value=32, max_value=512, step=32)\n",
    "        concatenated_inputs = tf.keras.layers.Dense(\n",
    "            units=units,\n",
    "            activation=activation_choice,\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(hp.Float('l2_{i}', 1e-5, 1e-2, sampling='log'))\n",
    "        )(concatenated_inputs)\n",
    "        if hp.Boolean(f'dropout_{i}'):\n",
    "            dropout_rate = hp.Float(f'dropout_rate_{i}', 0.1, 0.5)\n",
    "            concatenated_inputs = tf.keras.layers.Dropout(dropout_rate)(concatenated_inputs)\n",
    "\n",
    "    # Output layer for regression\n",
    "    output = tf.keras.layers.Dense(1, activation='linear')(concatenated_inputs)\n",
    "\n",
    "    # Create and compile the Keras model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "        loss='mean_squared_error',\n",
    "        metrics=[\n",
    "            tf.keras.metrics.MeanAbsoluteError(),\n",
    "            tf.keras.metrics.RootMeanSquaredError()\n",
    "        ]\n",
    "    )\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def tuner_fn(fn_args: FnArgs) -> TunerFnResult:\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
    "    tuner = RandomSearch(\n",
    "        hypermodel=lambda hp: _build_keras_model(hp, tf_transform_output),\n",
    "        objective='val_mean_absolute_error',\n",
    "        max_trials=25,\n",
    "        executions_per_trial=1,\n",
    "        directory=fn_args.working_dir,\n",
    "        project_name='taxi_trips_tuning_RandomSearch'\n",
    "    )\n",
    "\n",
    "    train_dataset = _input_fn(fn_args.train_files, fn_args.data_accessor, tf_transform_output, _BATCH_SIZE)\n",
    "    eval_dataset = _input_fn(fn_args.eval_files, fn_args.data_accessor, tf_transform_output, _BATCH_SIZE)\n",
    "\n",
    "    return TunerFnResult(\n",
    "        tuner=tuner,\n",
    "        fit_kwargs={\n",
    "            'x': train_dataset,\n",
    "            'validation_data': eval_dataset,\n",
    "            'steps_per_epoch': 1500,  \n",
    "            'validation_steps': 969,  \n",
    "            'callbacks': [early_stopping]\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def _get_tf_examples_serving_signature(model, tf_transform_output):\n",
    "    model.tft_layer_inference = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n",
    "    ])\n",
    "    def serve_tf_examples_fn(serialized_tf_example):\n",
    "        # Get the raw feature spec\n",
    "        raw_feature_spec = tf_transform_output.raw_feature_spec()\n",
    "\n",
    "        # Parse the raw features from the serialized example\n",
    "        raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n",
    "\n",
    "        # Exclude the 'demand' feature if it's present\n",
    "        if 'demand' in raw_features:\n",
    "            raw_features.pop('demand')\n",
    "\n",
    "        # Apply the transformation to get the features for model prediction\n",
    "        transformed_features = model.tft_layer_inference(raw_features)\n",
    "\n",
    "        # Filter out any keys not used by the model\n",
    "        model_input_keys = [layer.name for layer in model.layers if isinstance(layer, tf.keras.layers.InputLayer)]\n",
    "        filtered_features = {key: value for key, value in transformed_features.items() if key in model_input_keys}\n",
    "\n",
    "        # Make predictions with only the required inputs\n",
    "        outputs = model(filtered_features)\n",
    "        return {'outputs': outputs}\n",
    "\n",
    "    return serve_tf_examples_fn\n",
    "\n",
    "\n",
    "def _get_transform_features_signature(model, tf_transform_output):\n",
    "    model.tft_layer_eval = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n",
    "    ])\n",
    "    def transform_features_fn(serialized_tf_example):\n",
    "        raw_feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n",
    "        transformed_features = model.tft_layer_eval(raw_features)\n",
    "        return transformed_features\n",
    "\n",
    "    return transform_features_fn\n",
    "\n",
    "def export_serving_model(tf_transform_output, model, output_dir):\n",
    "    print(f\"Exporting the serving model to {output_dir}\")\n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "    signatures = {\n",
    "        'serving_default': _get_tf_examples_serving_signature(model, tf_transform_output),\n",
    "        'transform_features': _get_transform_features_signature(model, tf_transform_output),\n",
    "    }\n",
    "    model.save(output_dir, save_format='tf', signatures=signatures)\n",
    "    print(\"Model exported successfully.\")\n",
    "\n",
    "    \n",
    "def run_fn(fn_args: FnArgs):\n",
    "    print(f\"Model run directory: {fn_args.model_run_dir}\")\n",
    "    \n",
    "    model_dir = os.path.join(fn_args.model_run_dir, 'model')\n",
    "\n",
    "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_mean_absolute_error', \n",
    "        mode='min', \n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    \n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(model_dir, 'best_model'),\n",
    "        monitor='val_mean_absolute_error', \n",
    "        mode='min', \n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    " \n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
    "\n",
    "    # Set up the tuner for hyperparameter tuning\n",
    "    tuner_fn_result = tuner_fn(fn_args)\n",
    "    tuner = tuner_fn_result.tuner\n",
    "\n",
    "    # Create datasets for training and evaluation\n",
    "    train_dataset = _input_fn(\n",
    "        fn_args.train_files, \n",
    "        fn_args.data_accessor, \n",
    "        tf_transform_output, \n",
    "        _BATCH_SIZE\n",
    "    )\n",
    "    eval_dataset = _input_fn(\n",
    "        fn_args.eval_files, \n",
    "        fn_args.data_accessor, \n",
    "        tf_transform_output, \n",
    "        _BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    # Perform hyperparameter search using the tuner\n",
    "    tuner.search(**tuner_fn_result.fit_kwargs)\n",
    "\n",
    "    # Retrieve the best hyperparameters\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    # Build the best Keras model based on the best hyperparameters\n",
    "    model = _build_keras_model(best_hps, tf_transform_output)\n",
    "\n",
    "    # Determine the number of steps per epoch\n",
    "    total_train_examples = fn_args.train_steps * _BATCH_SIZE\n",
    "    steps_per_epoch = total_train_examples // _BATCH_SIZE\n",
    "\n",
    "    # Fit the model with the callbacks\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=eval_dataset,\n",
    "        validation_steps=fn_args.eval_steps,\n",
    "        epochs=fn_args.train_steps // steps_per_epoch,\n",
    "        callbacks=[\n",
    "            # tensorboard_callback,\n",
    "            early_stopping_callback,\n",
    "            model_checkpoint_callback\n",
    "        ]\n",
    "    )\n",
    "  \n",
    "    signatures = {\n",
    "      'serving_default': _get_tf_examples_serving_signature(model, tf_transform_output),\n",
    "    }\n",
    "    \n",
    "\n",
    "    # At the end of the run_fn function\n",
    "    export_serving_model(tf_transform_output, model, fn_args.serving_model_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57335c5e-2942-442e-a54e-4663c6771f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://taxi_trainer.py [Content-Type=text/x-python]...\n",
      "/ [1 files][ 10.1 KiB/ 10.1 KiB]                                                \n",
      "Operation completed over 1 objects/10.1 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp {_taxi_trainer_module_file} {MODULE_ROOT}/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4531e8f2-6d1b-4d48-9362-a9687576c76b",
   "metadata": {},
   "source": [
    "***Pipeline Definition***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a900868-d012-4d87-b1d5-a9dc5402c841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow_model_analysis as tfma\n",
    "import tfx\n",
    "from tfx import v1 as tfx\n",
    "from tfx.components import CsvExampleGen, StatisticsGen, SchemaGen, ExampleValidator, Transform, Trainer, Pusher, Evaluator, Tuner\n",
    "from tfx.orchestration import pipeline\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tfx.proto import example_gen_pb2, trainer_pb2, pusher_pb2\n",
    "from tfx.dsl.components.common.resolver import Resolver\n",
    "from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import LatestBlessedModelStrategy\n",
    "from keras_tuner.engine import base_tuner\n",
    "from keras_tuner import HyperParameters, RandomSearch\n",
    "from tfx.types import Channel\n",
    "from tfx.types.standard_artifacts import Model, ModelBlessing\n",
    "import taxi_constants\n",
    "\n",
    "def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
    "                     module_file: str, endpoint_name: str, project_id: str,\n",
    "                     region: str, serving_model_dir: str) -> tfx.dsl.Pipeline:\n",
    "    # Define input data\n",
    "    input_config = example_gen_pb2.Input(splits=[\n",
    "        example_gen_pb2.Input.Split(name='train', pattern=TRAIN_DATA_FILENAME),\n",
    "        example_gen_pb2.Input.Split(name='eval', pattern=EVAL_DATA_FILENAME)\n",
    "    ])\n",
    "    example_gen = CsvExampleGen(input_base=data_root, input_config=input_config)\n",
    "\n",
    "    # Components for data analysis and validation\n",
    "    statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])\n",
    "    schema_gen = SchemaGen(statistics=statistics_gen.outputs['statistics'])\n",
    "    example_validator = ExampleValidator(\n",
    "        statistics=statistics_gen.outputs['statistics'],\n",
    "        schema=schema_gen.outputs['schema']\n",
    "    )\n",
    "\n",
    "    # Component for data transformation\n",
    "    transform = Transform(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        module_file=os.path.abspath(TRANSFORM_MODULE_PATH)\n",
    "    )\n",
    "    \n",
    "    # Configuration for Vertex AI Training\n",
    "    vertex_job_spec = {\n",
    "        'project': project_id,\n",
    "        'worker_pool_specs': [{\n",
    "            'machine_spec': {\n",
    "                'machine_type': 'n1-standard-4'\n",
    "            },\n",
    "            'replica_count': 1,\n",
    "            'container_spec': {\n",
    "                'image_uri': 'gcr.io/tfx-oss-public/tfx:{}'.format(tfx.__version__),\n",
    "            },\n",
    "        }],\n",
    "    }\n",
    "    \n",
    "    # Tuner component\n",
    "    tuner = tfx.extensions.google_cloud_ai_platform.Tuner(\n",
    "        module_file=_taxi_trainer_module_file,\n",
    "        examples=transform.outputs['transformed_examples'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        transform_graph=transform.outputs['transform_graph'],\n",
    "        train_args=trainer_pb2.TrainArgs(splits=['train'], num_steps=100),\n",
    "        eval_args=trainer_pb2.EvalArgs(splits=['eval'], num_steps=5),\n",
    "         custom_config={\n",
    "        tfx.extensions.google_cloud_ai_platform.ENABLE_VERTEX_KEY: True,\n",
    "        tfx.extensions.google_cloud_ai_platform.VERTEX_REGION_KEY: region,\n",
    "        tfx.extensions.google_cloud_ai_platform.experimental.TUNING_ARGS_KEY: vertex_job_spec\n",
    "        # tfx.extensions.google_cloud_ai_platform.experimental.REMOTE_TRIALS_WORKING_DIR_KEY: os.path.join('gs://', GCS_BUCKET_NAME, 'tuner_trials')\n",
    "    }\n",
    "   )\n",
    "    \n",
    "    \n",
    "    # Trainer component\n",
    "    trainer = tfx.extensions.google_cloud_ai_platform.Trainer(\n",
    "        module_file=os.path.abspath(module_file),\n",
    "        transformed_examples=transform.outputs['transformed_examples'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        transform_graph=transform.outputs['transform_graph'],\n",
    "        hyperparameters=tuner.outputs['best_hyperparameters'],\n",
    "        train_args=tfx.proto.TrainArgs(splits=['train'], num_steps=10160),\n",
    "        eval_args=tfx.proto.EvalArgs(splits=['eval'], num_steps=5716),\n",
    "        custom_config={\n",
    "          tfx.extensions.google_cloud_ai_platform.ENABLE_VERTEX_KEY:\n",
    "              True,\n",
    "          tfx.extensions.google_cloud_ai_platform.VERTEX_REGION_KEY:\n",
    "              region,\n",
    "          tfx.extensions.google_cloud_ai_platform.TRAINING_ARGS_KEY:\n",
    "              vertex_job_spec\n",
    "    \n",
    "        }\n",
    "    )\n",
    "    \n",
    "    vertex_serving_spec = {\n",
    "      'project_id': project_id,\n",
    "      'endpoint_name': endpoint_name,\n",
    "      'machine_type': 'n1-standard-4',\n",
    "  }\n",
    "    # serving_image = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-12:latest'\n",
    "    serving_image = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-13:latest'\n",
    "\n",
    "    # Evaluator component configuration\n",
    "    eval_config = tfma.EvalConfig(\n",
    "        model_specs=[tfma.ModelSpec(signature_name='serving_default', label_key=taxi_constants.LABEL_KEY)],\n",
    "        slicing_specs=[tfma.SlicingSpec()],\n",
    "        metrics_specs=[\n",
    "            tfma.MetricsSpec(\n",
    "                metrics=[\n",
    "                    tfma.MetricConfig(class_name='MeanAbsoluteError'),\n",
    "                    tfma.MetricConfig(\n",
    "                        class_name='MeanAbsoluteError',\n",
    "                        threshold=tfma.MetricThreshold(\n",
    "                            value_threshold=tfma.GenericValueThreshold(lower_bound={'value': 0.8159}),\n",
    "                            change_threshold=tfma.GenericChangeThreshold(\n",
    "                                direction=tfma.MetricDirection.LOWER_IS_BETTER,\n",
    "                                absolute={'value': 0.02}\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Model resolver component\n",
    "    model_resolver = Resolver(\n",
    "        strategy_class=LatestBlessedModelStrategy,\n",
    "        model=Channel(type=Model),\n",
    "        model_blessing=Channel(type=ModelBlessing)\n",
    "    ).with_id('latest_blessed_model_resolver')\n",
    "\n",
    "    # Evaluator component\n",
    "    evaluator = Evaluator(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        example_splits=['eval'],  \n",
    "        model=trainer.outputs['model'],\n",
    "        baseline_model=model_resolver.outputs['model'],\n",
    "        eval_config=eval_config\n",
    "    )\n",
    "\n",
    "    # Pusher component\n",
    "    pusher = tfx.extensions.google_cloud_ai_platform.Pusher(\n",
    "        model=trainer.outputs['model'],\n",
    "        model_blessing=evaluator.outputs['blessing'],\n",
    "        custom_config={\n",
    "          tfx.extensions.google_cloud_ai_platform.ENABLE_VERTEX_KEY:\n",
    "              True,\n",
    "          tfx.extensions.google_cloud_ai_platform.VERTEX_REGION_KEY:\n",
    "              region,\n",
    "          tfx.extensions.google_cloud_ai_platform.VERTEX_CONTAINER_IMAGE_URI_KEY:\n",
    "              serving_image,\n",
    "          tfx.extensions.google_cloud_ai_platform.SERVING_ARGS_KEY:\n",
    "            vertex_serving_spec,\n",
    "        })\n",
    "\n",
    "    # Define the pipeline\n",
    "    components = [\n",
    "        example_gen,\n",
    "        statistics_gen,\n",
    "        schema_gen,\n",
    "        example_validator,\n",
    "        transform,\n",
    "        tuner,\n",
    "        trainer,\n",
    "        model_resolver,\n",
    "        evaluator,\n",
    "        pusher\n",
    "    ]\n",
    "\n",
    "    return tfx.dsl.Pipeline(\n",
    "        pipeline_name=pipeline_name,\n",
    "        pipeline_root=pipeline_root,\n",
    "        components=components\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4db35d21-aad6-466a-a5d8-fa6c9670ea0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7335573c-fb2e-41b3-b6bb-9bb757996298",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`transformed_examples` is deprecated. Please use `examples` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying chicago_taxi_transform.py -> build/lib\n",
      "copying taxi_transform.py -> build/lib\n",
      "copying taxi_trainer.py -> build/lib\n",
      "copying taxi_constants.py -> build/lib\n",
      "copying taxi_tuner.py -> build/lib\n",
      "installing to /tmp/tmpx5_h3zx2\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/taxi_transform.py -> /tmp/tmpx5_h3zx2\n",
      "copying build/lib/taxi_tuner.py -> /tmp/tmpx5_h3zx2\n",
      "copying build/lib/taxi_constants.py -> /tmp/tmpx5_h3zx2\n",
      "copying build/lib/taxi_trainer.py -> /tmp/tmpx5_h3zx2\n",
      "copying build/lib/chicago_taxi_transform.py -> /tmp/tmpx5_h3zx2\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_Transform.egg-info\n",
      "writing tfx_user_code_Transform.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_Transform.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_Transform.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_Transform.egg-info to /tmp/tmpx5_h3zx2/tfx_user_code_Transform-0.0+38c9a72244295a98276b06a0cc23c38920807fdf5f9a424850ed1523130d21de-py3.10.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmpx5_h3zx2/tfx_user_code_Transform-0.0+38c9a72244295a98276b06a0cc23c38920807fdf5f9a424850ed1523130d21de.dist-info/WHEEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` directly.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating '/tmp/tmpc_lawmam/tfx_user_code_Transform-0.0+38c9a72244295a98276b06a0cc23c38920807fdf5f9a424850ed1523130d21de-py3-none-any.whl' and adding '/tmp/tmpx5_h3zx2' to it\n",
      "adding 'chicago_taxi_transform.py'\n",
      "adding 'taxi_constants.py'\n",
      "adding 'taxi_trainer.py'\n",
      "adding 'taxi_transform.py'\n",
      "adding 'taxi_tuner.py'\n",
      "adding 'tfx_user_code_Transform-0.0+38c9a72244295a98276b06a0cc23c38920807fdf5f9a424850ed1523130d21de.dist-info/METADATA'\n",
      "adding 'tfx_user_code_Transform-0.0+38c9a72244295a98276b06a0cc23c38920807fdf5f9a424850ed1523130d21de.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_Transform-0.0+38c9a72244295a98276b06a0cc23c38920807fdf5f9a424850ed1523130d21de.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_Transform-0.0+38c9a72244295a98276b06a0cc23c38920807fdf5f9a424850ed1523130d21de.dist-info/RECORD'\n",
      "removing /tmp/tmpx5_h3zx2\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying chicago_taxi_transform.py -> build/lib\n",
      "copying taxi_transform.py -> build/lib\n",
      "copying taxi_trainer.py -> build/lib\n",
      "copying taxi_constants.py -> build/lib\n",
      "copying taxi_tuner.py -> build/lib\n",
      "installing to /tmp/tmp91m96p5_\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/taxi_transform.py -> /tmp/tmp91m96p5_\n",
      "copying build/lib/taxi_tuner.py -> /tmp/tmp91m96p5_\n",
      "copying build/lib/taxi_constants.py -> /tmp/tmp91m96p5_\n",
      "copying build/lib/taxi_trainer.py -> /tmp/tmp91m96p5_\n",
      "copying build/lib/chicago_taxi_transform.py -> /tmp/tmp91m96p5_\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_Tuner.egg-info\n",
      "writing tfx_user_code_Tuner.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_Tuner.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_Tuner.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_Tuner.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_Tuner.egg-info/SOURCES.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` directly.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing manifest file 'tfx_user_code_Tuner.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_Tuner.egg-info to /tmp/tmp91m96p5_/tfx_user_code_Tuner-0.0+38c9a72244295a98276b06a0cc23c38920807fdf5f9a424850ed1523130d21de-py3.10.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmp91m96p5_/tfx_user_code_Tuner-0.0+38c9a72244295a98276b06a0cc23c38920807fdf5f9a424850ed1523130d21de.dist-info/WHEEL\n",
      "creating '/tmp/tmp1otndb59/tfx_user_code_Tuner-0.0+38c9a72244295a98276b06a0cc23c38920807fdf5f9a424850ed1523130d21de-py3-none-any.whl' and adding '/tmp/tmp91m96p5_' to it\n",
      "adding 'chicago_taxi_transform.py'\n",
      "adding 'taxi_constants.py'\n",
      "adding 'taxi_trainer.py'\n",
      "adding 'taxi_transform.py'\n",
      "adding 'taxi_tuner.py'\n",
      "adding 'tfx_user_code_Tuner-0.0+38c9a72244295a98276b06a0cc23c38920807fdf5f9a424850ed1523130d21de.dist-info/METADATA'\n",
      "adding 'tfx_user_code_Tuner-0.0+38c9a72244295a98276b06a0cc23c38920807fdf5f9a424850ed1523130d21de.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_Tuner-0.0+38c9a72244295a98276b06a0cc23c38920807fdf5f9a424850ed1523130d21de.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_Tuner-0.0+38c9a72244295a98276b06a0cc23c38920807fdf5f9a424850ed1523130d21de.dist-info/RECORD'\n",
      "removing /tmp/tmp91m96p5_\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying chicago_taxi_transform.py -> build/lib\n",
      "copying taxi_transform.py -> build/lib\n",
      "copying taxi_trainer.py -> build/lib\n",
      "copying taxi_constants.py -> build/lib\n",
      "copying taxi_tuner.py -> build/lib\n",
      "installing to /tmp/tmp8xcbqs_2\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/taxi_transform.py -> /tmp/tmp8xcbqs_2\n",
      "copying build/lib/taxi_tuner.py -> /tmp/tmp8xcbqs_2\n",
      "copying build/lib/taxi_constants.py -> /tmp/tmp8xcbqs_2\n",
      "copying build/lib/taxi_trainer.py -> /tmp/tmp8xcbqs_2\n",
      "copying build/lib/chicago_taxi_transform.py -> /tmp/tmp8xcbqs_2\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_Trainer.egg-info\n",
      "writing tfx_user_code_Trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_Trainer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_Trainer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_Trainer.egg-info to /tmp/tmp8xcbqs_2/tfx_user_code_Trainer-0.0+38c9a72244295a98276b06a0cc23c38920807fdf5f9a424850ed1523130d21de-py3.10.egg-info\n",
      "running install_scripts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` directly.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating /tmp/tmp8xcbqs_2/tfx_user_code_Trainer-0.0+38c9a72244295a98276b06a0cc23c38920807fdf5f9a424850ed1523130d21de.dist-info/WHEEL\n",
      "creating '/tmp/tmprhq92uo3/tfx_user_code_Trainer-0.0+38c9a72244295a98276b06a0cc23c38920807fdf5f9a424850ed1523130d21de-py3-none-any.whl' and adding '/tmp/tmp8xcbqs_2' to it\n",
      "adding 'chicago_taxi_transform.py'\n",
      "adding 'taxi_constants.py'\n",
      "adding 'taxi_trainer.py'\n",
      "adding 'taxi_transform.py'\n",
      "adding 'taxi_tuner.py'\n",
      "adding 'tfx_user_code_Trainer-0.0+38c9a72244295a98276b06a0cc23c38920807fdf5f9a424850ed1523130d21de.dist-info/METADATA'\n",
      "adding 'tfx_user_code_Trainer-0.0+38c9a72244295a98276b06a0cc23c38920807fdf5f9a424850ed1523130d21de.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_Trainer-0.0+38c9a72244295a98276b06a0cc23c38920807fdf5f9a424850ed1523130d21de.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_Trainer-0.0+38c9a72244295a98276b06a0cc23c38920807fdf5f9a424850ed1523130d21de.dist-info/RECORD'\n",
      "removing /tmp/tmp8xcbqs_2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PIPELINE_DEFINITION_FILE = PIPELINE_NAME + '_pipeline.json'\n",
    "\n",
    "runner = tfx.orchestration.experimental.KubeflowV2DagRunner(\n",
    "    config=tfx.orchestration.experimental.KubeflowV2DagRunnerConfig(),\n",
    "    output_filename=PIPELINE_DEFINITION_FILE)\n",
    "# Following function will write the pipeline definition to PIPELINE_DEFINITION_FILE.\n",
    "_ = runner.run(\n",
    "    _create_pipeline(\n",
    "        pipeline_name=PIPELINE_NAME,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        data_root=DATA_DIRECTORY,\n",
    "        module_file=_taxi_trainer_module_file,\n",
    "        endpoint_name=ENDPOINT_NAME,\n",
    "        project_id=GOOGLE_CLOUD_PROJECT,\n",
    "        region=GOOGLE_CLOUD_REGION,\n",
    "        serving_model_dir=SERVING_MODEL_DIR,\n",
    "        \n",
    "       \n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa18c984-d8b5-4f09-ba8b-574a83d2682c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/75674212269/locations/us-central1/pipelineJobs/taxi-demand-prediction-management-20231213030329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/75674212269/locations/us-central1/pipelineJobs/taxi-demand-prediction-management-20231213030329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_job = aiplatform.PipelineJob.get('projects/75674212269/locations/us-central1/pipelineJobs/taxi-demand-prediction-management-20231213030329')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/75674212269/locations/us-central1/pipelineJobs/taxi-demand-prediction-management-20231213030329')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/taxi-demand-prediction-management-20231213030329?project=75674212269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/taxi-demand-prediction-management-20231213030329?project=75674212269\n"
     ]
    }
   ],
   "source": [
    "# docs_infra: no_execute\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "aiplatform.init(project=GOOGLE_CLOUD_PROJECT, location=GOOGLE_CLOUD_REGION)\n",
    "\n",
    "job = pipeline_jobs.PipelineJob(template_path=PIPELINE_DEFINITION_FILE,\n",
    "                                display_name=PIPELINE_NAME)\n",
    "job.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16191122-7b1b-4ac9-b3de-3650c45107d7",
   "metadata": {},
   "source": [
    "----\n",
    "<h3 style='color:darkblue;text-align:center;'><strong>Conclusion and Future Directions</strong></h3>\n",
    "<p style='color:black; font-size:16px; text-align:justify;'>\n",
    "Our journey in developing the Taxi Demand Prediction and Management project has been a significant step forward in integrating advanced analytics and machine learning into practical applications. With this latest update, we are initiating the deployment of our pipeline in a cloud environment, utilizing the resources and services available through Vertex AI. This is an important phase in making our Taxi Demand Prediction model more scalable and robust, ready for real-world challenges in urban transport.\n",
    "\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "<strong>Key Achievements:</strong>\n",
    "- <em style='color:darkgreen;'>Enhanced Operational Efficiency:</em> Our predictive model significantly improves taxi distribution, aligning with our operational goal of optimizing resource allocation and ensuring availability during peak demand.\n",
    "- <em style='color:darkgreen;'>Streamlined Taxi Distribution:</em> Focused on reducing customer wait times, our project has successfully laid the groundwork for responsive and efficient taxi services.\n",
    "- <em style='color:darkgreen;'>Contribution to Urban Mobility:</em> Beyond taxi services, our work contributes to the broader urban transportation ecosystem, enhancing overall urban mobility.\n",
    "- <em style='color:darkgreen;'>Foundation for Future Exploration:</em> We've established a solid base for ongoing advancements in AI and machine learning, showcasing the potential for real-world applications and industry-wide transformations.\n",
    "\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a663c08-53c8-46ae-85c0-123e06de8b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2.12 (Local)",
   "language": "python",
   "name": "local-tf2-2-12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
