{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c4f81b-8eea-4954-b0d1-d33fda575088",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3 style='color:darkblue; text-align:center;'><strong>Enriching Chicago Taxi Trip Dataset with Weather Data</strong></h3>\n",
    "<p style='color:black; font-size:16px; text-align:justify;'>\n",
    "In this project, we utilize the Open-Meteo.com Weather API to enhance our analysis of the Chicago Taxi Trip dataset. By integrating weather data such as temperature, humidity, and precipitation, we aim to uncover insightful correlations between weather conditions and taxi trip patterns. This integration allows for a more comprehensive understanding of factors influencing taxi demand in Chicago. The Open-Meteo API provides reliable, up-to-date weather information, making it an invaluable resource for our analysis. Acknowledging Open-Meteo as the source of our weather data, we adhere to the principles of transparency and accuracy.\n",
    "<br><br>\n",
    "<strong style='color:darkblue;'>Citation (APA):</strong> Zippenfenig, P. (2023). <strong style='color:darkblue;'>Open-Meteo.com Weather API</strong> [Computer software]. Zenodo. <a href=\"https://doi.org/10.5281/ZENODO.7970649\" style='color:darkblue;'><strong>https://doi.org/10.5281/ZENODO.7970649</strong></a>\n",
    "</p>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824008c2-02a0-4bc7-bfd2-966b3a4602f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Fetch the data from the API\n",
    "response = requests.get('https://archive-api.open-meteo.com/v1/archive?latitude=41.85&longitude=-87.65&start_date=2013-01-11&end_date=2023-09-10&hourly=temperature_2m,relativehumidity_2m,precipitation,rain,snowfall,weathercode,windspeed_10m&models=best_match&daily=weathercode,temperature_2m_max,temperature_2m_min,temperature_2m_mean,shortwave_radiation_sum,precipitation_sum,rain_sum,snowfall_sum,precipitation_hours,windspeed_10m_max&timezone=America%2FChicago&min=2013-01-01&max=2023-09-10')\n",
    "\n",
    "# Load the JSON response\n",
    "data = response.json()\n",
    "\n",
    "# Create DataFrames from the 'hourly' and 'daily' data\n",
    "df_hourly = pd.DataFrame(data['hourly'])\n",
    "df_daily = pd.DataFrame(data['daily'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035616b6-0831-48e4-8e56-e6a7dc076d8d",
   "metadata": {},
   "source": [
    "\n",
    "<span style='color:darkblue'><strong>Comment:</strong></span> <span style='color:black'> The purpose of this cell is to set up the environment by importing libraries and to fetch and structure weather data from the Open-Meteo API into DataFrames for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96afaf1-a156-4783-ad32-2b6325337f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [temperature_2m, relativehumidity_2m, precipitation, rain, snowfall, weathercode_x, windspeed_10m, weathercode_y, temperature_2m_max, temperature_2m_min, temperature_2m_mean, shortwave_radiation_sum, precipitation_sum, rain_sum, snowfall_sum, precipitation_hours, windspeed_10m_max]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Set the indices of the DataFrames to be the timestamps\n",
    "df_hourly.set_index('time', inplace=True)\n",
    "df_daily.set_index('time', inplace=True)\n",
    "\n",
    "# Merge the DataFrames\n",
    "df_merged = pd.merge(df_hourly, df_daily, left_index=True, right_index=True)\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(df_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a79b5c-6dd8-4885-baf5-3309351eb1bc",
   "metadata": {},
   "source": [
    "<span style='color:darkblue'><strong>Comment:</strong></span> <span style='color:black'>This code block strategically organizes and integrates our weather data. It starts by setting the 'time' column as the index for both hourly and daily weather DataFrames, ensuring alignment on a time basis. We then merge these DataFrames, creating a comprehensive dataset that combines both hourly and daily weather insights. This integrated dataset is key for in-depth analysis of weather patterns and their influence on taxi trip demand.</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be9f70e-fcba-4564-af4e-50c9b244d34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature_2m</th>\n",
       "      <th>relativehumidity_2m</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>rain</th>\n",
       "      <th>snowfall</th>\n",
       "      <th>weathercode</th>\n",
       "      <th>windspeed_10m</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-11T00:00</th>\n",
       "      <td>4.6</td>\n",
       "      <td>98</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-11T01:00</th>\n",
       "      <td>5.5</td>\n",
       "      <td>98</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>21.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-11T02:00</th>\n",
       "      <td>5.5</td>\n",
       "      <td>98</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>21.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-11T03:00</th>\n",
       "      <td>6.5</td>\n",
       "      <td>99</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61</td>\n",
       "      <td>22.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-11T04:00</th>\n",
       "      <td>7.6</td>\n",
       "      <td>99</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>24.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  temperature_2m  relativehumidity_2m  precipitation  rain  \\\n",
       "time                                                                         \n",
       "2013-01-11T00:00             4.6                   98            2.4   2.4   \n",
       "2013-01-11T01:00             5.5                   98            1.2   1.2   \n",
       "2013-01-11T02:00             5.5                   98            1.1   1.1   \n",
       "2013-01-11T03:00             6.5                   99            1.8   1.8   \n",
       "2013-01-11T04:00             7.6                   99            1.1   1.1   \n",
       "\n",
       "                  snowfall  weathercode  windspeed_10m  \n",
       "time                                                    \n",
       "2013-01-11T00:00       0.0           61           22.4  \n",
       "2013-01-11T01:00       0.0           55           21.1  \n",
       "2013-01-11T02:00       0.0           55           21.4  \n",
       "2013-01-11T03:00       0.0           61           22.5  \n",
       "2013-01-11T04:00       0.0           55           24.3  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hourly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6fe410-88ac-4845-ad8d-1a3d9e9861ff",
   "metadata": {},
   "source": [
    "<span style='color:darkblue'><strong>Comment:</strong></span> <span style='color:black'>This line of code displays the first five rows of the `df_hourly` DataFrame. It's a simple yet effective way to quickly inspect the structure and the initial entries of our hourly weather data, ensuring that the data is loaded and formatted correctly.</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34858906-e201-4fe0-b935-20ce82f73237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  temperature_2m  relativehumidity_2m  precipitation  rain  \\\n",
      "time                                                                         \n",
      "2013-01-11T00:00             4.6                   98            2.4   2.4   \n",
      "2013-01-11T01:00             5.5                   98            1.2   1.2   \n",
      "2013-01-11T02:00             5.5                   98            1.1   1.1   \n",
      "2013-01-11T03:00             6.5                   99            1.8   1.8   \n",
      "2013-01-11T04:00             7.6                   99            1.1   1.1   \n",
      "2013-01-11T05:00             8.6                   99            0.3   0.3   \n",
      "2013-01-11T06:00            10.3                   99            0.3   0.3   \n",
      "2013-01-11T07:00            10.5                   99            0.2   0.2   \n",
      "2013-01-11T08:00            10.4                   99            0.6   0.6   \n",
      "2013-01-11T09:00             9.9                   99            0.4   0.4   \n",
      "\n",
      "                  snowfall  weathercode  windspeed_10m  \n",
      "time                                                    \n",
      "2013-01-11T00:00       0.0           61           22.4  \n",
      "2013-01-11T01:00       0.0           55           21.1  \n",
      "2013-01-11T02:00       0.0           55           21.4  \n",
      "2013-01-11T03:00       0.0           61           22.5  \n",
      "2013-01-11T04:00       0.0           55           24.3  \n",
      "2013-01-11T05:00       0.0           51           22.7  \n",
      "2013-01-11T06:00       0.0           51           22.7  \n",
      "2013-01-11T07:00       0.0           51           21.7  \n",
      "2013-01-11T08:00       0.0           53           22.3  \n",
      "2013-01-11T09:00       0.0           51           22.4  \n"
     ]
    }
   ],
   "source": [
    "print(df_hourly.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98179f62-edc6-4225-83f0-cf5f6c62b640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature_2m         float64\n",
      "relativehumidity_2m      int64\n",
      "precipitation          float64\n",
      "rain                   float64\n",
      "snowfall               float64\n",
      "weathercode              int64\n",
      "windspeed_10m          float64\n",
      "dtype: object\n",
      "Index(['2013-01-11T00:00', '2013-01-11T01:00', '2013-01-11T02:00',\n",
      "       '2013-01-11T03:00', '2013-01-11T04:00', '2013-01-11T05:00',\n",
      "       '2013-01-11T06:00', '2013-01-11T07:00', '2013-01-11T08:00',\n",
      "       '2013-01-11T09:00',\n",
      "       ...\n",
      "       '2023-09-10T14:00', '2023-09-10T15:00', '2023-09-10T16:00',\n",
      "       '2023-09-10T17:00', '2023-09-10T18:00', '2023-09-10T19:00',\n",
      "       '2023-09-10T20:00', '2023-09-10T21:00', '2023-09-10T22:00',\n",
      "       '2023-09-10T23:00'],\n",
      "      dtype='object', name='time', length=93480)\n"
     ]
    }
   ],
   "source": [
    "print(df_hourly.dtypes)\n",
    "print(df_hourly.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4972970f-98da-4fb1-800b-a3ee04e24acc",
   "metadata": {},
   "source": [
    "***Checking the data types***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd70d43-3749-4dd9-ae73-f13cde5210c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  temperature_2m  relativehumidity_2m  precipitation   rain  \\\n",
      "time                                                                          \n",
      "2013-01-11T00:00           False                False          False  False   \n",
      "2013-01-11T01:00           False                False          False  False   \n",
      "2013-01-11T02:00           False                False          False  False   \n",
      "2013-01-11T03:00           False                False          False  False   \n",
      "2013-01-11T04:00           False                False          False  False   \n",
      "...                          ...                  ...            ...    ...   \n",
      "2023-09-10T19:00           False                False          False  False   \n",
      "2023-09-10T20:00           False                False          False  False   \n",
      "2023-09-10T21:00           False                False          False  False   \n",
      "2023-09-10T22:00           False                False          False  False   \n",
      "2023-09-10T23:00           False                False          False  False   \n",
      "\n",
      "                  snowfall  weathercode  windspeed_10m  \n",
      "time                                                    \n",
      "2013-01-11T00:00     False        False          False  \n",
      "2013-01-11T01:00     False        False          False  \n",
      "2013-01-11T02:00     False        False          False  \n",
      "2013-01-11T03:00     False        False          False  \n",
      "2013-01-11T04:00     False        False          False  \n",
      "...                    ...          ...            ...  \n",
      "2023-09-10T19:00     False        False          False  \n",
      "2023-09-10T20:00     False        False          False  \n",
      "2023-09-10T21:00     False        False          False  \n",
      "2023-09-10T22:00     False        False          False  \n",
      "2023-09-10T23:00     False        False          False  \n",
      "\n",
      "[93480 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "nan_df = df_hourly.isna()\n",
    "print(nan_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a20335-7efb-4845-ad22-adf51f08d6b8",
   "metadata": {},
   "source": [
    "<span style='color:darkblue'><strong>Comment:</strong></span> <span style='color:black'>This segment of code creates a DataFrame named `nan_df`, which is used to identify missing values (NaNs) in the `df_hourly` DataFrame. By applying the `isna()` method, each cell in `nan_df` corresponds to whether the respective cell in `df_hourly` is NaN or not. The `print(nan_df)` command then outputs this DataFrame, allowing for a quick examination of the presence and distribution of missing values in our hourly weather data.</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2805877b-aaa4-4eb9-9ee8-4ff8bcbbd586",
   "metadata": {},
   "source": [
    "***Checking for any null values***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985e1a27-77dc-461f-8597-b083a0470dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time\n",
      "2013-01-11T00:00    False\n",
      "2013-01-11T01:00    False\n",
      "2013-01-11T02:00    False\n",
      "2013-01-11T03:00    False\n",
      "2013-01-11T04:00    False\n",
      "                    ...  \n",
      "2023-09-10T19:00    False\n",
      "2023-09-10T20:00    False\n",
      "2023-09-10T21:00    False\n",
      "2023-09-10T22:00    False\n",
      "2023-09-10T23:00    False\n",
      "Length: 93480, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "nan_rows = df_hourly.isna().any(axis=1)\n",
    "print(nan_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9903db2b-e0f6-4f26-80fc-554b4f719966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows with NaN values: 0\n"
     ]
    }
   ],
   "source": [
    "nan_count = nan_rows.sum()\n",
    "print(f\"Total number of rows with NaN values: {nan_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8846a4b4-5830-4098-9636-e92a65711245",
   "metadata": {},
   "source": [
    "<span style='color:darkblue'><strong>Comment:</strong></span> <span style='color:black'>This code creates a Series named `nan_rows` from the `df_hourly` DataFrame. It applies the `isna()` method combined with `any(axis=1)` to identify any rows that contain at least one NaN (missing value). Each entry in `nan_rows` is a boolean indicating whether the corresponding row in `df_hourly` has any NaN values. The `print(nan_rows)` command outputs this Series, offering an overview of which rows in our hourly weather data have missing information.</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046f1d54-92b0-4098-8f8b-11fdba87928f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='object', name='time')\n"
     ]
    }
   ],
   "source": [
    "nan_dates = df_hourly.index[nan_rows]\n",
    "print(nan_dates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84adafa3-e990-4b8b-9fed-9ecba9533328",
   "metadata": {},
   "source": [
    "<span style='color:darkblue'><strong>Comment:</strong></span> <span style='color:black'>This code creates a Series named `nan_rows` from the `df_hourly` DataFrame. It applies the `isna()` method combined with `any(axis=1)` to identify any rows that contain at least one NaN (missing value). Each entry in `nan_rows` is a boolean indicating whether the corresponding row in `df_hourly` has any NaN values. The `print(nan_rows)` command outputs this Series, offering an overview of which rows in our hourly weather data have missing information.</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e34dd21-c077-40fc-a9d5-b7b9a72a0635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time\n",
      "2023-09-15 19:00:00    7\n",
      "2023-09-15 20:00:00    7\n",
      "2023-09-15 21:00:00    7\n",
      "2023-09-15 22:00:00    7\n",
      "2023-09-15 23:00:00    7\n",
      "                      ..\n",
      "2023-09-21 19:00:00    7\n",
      "2023-09-21 20:00:00    7\n",
      "2023-09-21 21:00:00    7\n",
      "2023-09-21 22:00:00    7\n",
      "2023-09-21 23:00:00    7\n",
      "Length: 149, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "nan_summary = df_hourly[nan_rows].isna().sum(axis=1)\n",
    "print(nan_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abf9b7c-e5ab-4a43-83c4-d43d40b1220b",
   "metadata": {},
   "source": [
    "<span style='color:darkblue'><strong>Comment:</strong></span> <span style='color:black'>This code snippet generates a summary of missing values in the `df_hourly` DataFrame. By indexing `df_hourly` with `nan_rows`, we focus on rows that have at least one NaN value. We then apply the `isna()` method followed by `sum(axis=1)`, which counts the number of NaNs in each of these rows. The resulting Series, `nan_summary`, provides a detailed view of the distribution of missing values across the selected rows. Finally, we output this summary using `print(nan_summary)`, offering a concise representation of NaN occurrences in specific rows of our dataset.</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f924eb26-8fc0-4a38-ac04-e2196266b2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['2013-01-11T00:00', '2013-01-11T01:00', '2013-01-11T02:00',\n",
      "       '2013-01-11T03:00', '2013-01-11T04:00', '2013-01-11T05:00',\n",
      "       '2013-01-11T06:00', '2013-01-11T07:00', '2013-01-11T08:00',\n",
      "       '2013-01-11T09:00',\n",
      "       ...\n",
      "       '2023-09-21T14:00', '2023-09-21T15:00', '2023-09-21T16:00',\n",
      "       '2023-09-21T17:00', '2023-09-21T18:00', '2023-09-21T19:00',\n",
      "       '2023-09-21T20:00', '2023-09-21T21:00', '2023-09-21T22:00',\n",
      "       '2023-09-21T23:00'],\n",
      "      dtype='object', name='time', length=93744)\n",
      "Index(['2013-01-11', '2013-01-12', '2013-01-13', '2013-01-14', '2013-01-15',\n",
      "       '2013-01-16', '2013-01-17', '2013-01-18', '2013-01-19', '2013-01-20',\n",
      "       ...\n",
      "       '2023-09-12', '2023-09-13', '2023-09-14', '2023-09-15', '2023-09-16',\n",
      "       '2023-09-17', '2023-09-18', '2023-09-19', '2023-09-20', '2023-09-21'],\n",
      "      dtype='object', name='time', length=3906)\n"
     ]
    }
   ],
   "source": [
    "print(df_hourly.index.unique())\n",
    "print(df_daily.index.unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e7eeb7-4e27-40bd-8dea-a972c6a714b0",
   "metadata": {},
   "source": [
    "<span style='color:darkblue'><strong>Comment:</strong></span> <span style='color:black'>This code block displays the unique index values for both the `df_hourly` and `df_daily` DataFrames. By calling `print(df_hourly.index.unique())`, we examine the unique timestamps in the hourly data. Similarly, `print(df_daily.index.unique())`, with the text in black, allows us to inspect the unique timestamps in the daily data. This comparison is crucial for understanding the temporal coverage and consistency between these two datasets.It helps in verifying that the indices align correctly, especially important when dealing with time series data where the sequence and continuity of dates and times are critical for accurate analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a03cf3-3609-4e2b-9271-614556b85e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=mlops-363723, location=US, id=6740581f-f62e-42ad-95ae-f1c3a3c968bd>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Initialize a BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Convert time columns to datetime\n",
    "df_hourly.index = pd.to_datetime(df_hourly.index)\n",
    "df_daily.index = pd.to_datetime(df_daily.index)\n",
    "\n",
    "# Upload the hourly DataFrame\n",
    "table_id = 'mlops-363723.ChicagoTaxitrips.weather_hourly'\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_TRUNCATE\",  # overwrite existing table\n",
    ")\n",
    "client.load_table_from_dataframe(df_hourly, table_id, job_config=job_config).result()\n",
    "\n",
    "# Upload the daily DataFrame\n",
    "table_id = 'mlops-363723.ChicagoTaxitrips.weather_daily'\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_TRUNCATE\",  # overwrite existing table\n",
    ")\n",
    "client.load_table_from_dataframe(df_daily, table_id, job_config=job_config).result()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c47ef5-122e-49c2-ac71-99f30f6884b6",
   "metadata": {},
   "source": [
    "<span style='color:darkblue'><strong>Comment:</strong></span> <span style='color:black'>This code initiates the process of uploading our weather data to Google BigQuery. It begins by initializing a BigQuery client to handle our data transactions. Next, the indices of both `df_hourly` and `df_daily` DataFrames are converted to datetime format, ensuring compatibility with BigQuery's timestamp requirements. The following steps involve uploading these DataFrames to BigQuery, with specific tables designated for hourly and daily data (`weather_hourly` and `weather_daily`). The use of `WRITE_TRUNCATE` in the job configuration is crucial as it allows the new data to replace any existing data in these tables, keeping our BigQuery dataset up-to-date and consistent.</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0289f1e2-9673-4e95-98ca-14f500d394eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded successfully to the test table.\n",
      "Data uploaded successfully to the main table.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Convert time columns to datetime and reset the index\n",
    "df_hourly.index = pd.to_datetime(df_hourly.index)\n",
    "df_hourly = df_hourly.reset_index()\n",
    "\n",
    "# Upload the hourly DataFrame to a new test table\n",
    "table_id_new = 'mlops-363723.ChicagoTaxitrips.weather_hourly_test'\n",
    "try:\n",
    "    client.load_table_from_dataframe(df_hourly, table_id_new).result()\n",
    "    print(\"Data uploaded successfully to the test table.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during upload: {e}\")\n",
    "\n",
    "# If the test table upload is successful and you want to overwrite the main table:\n",
    "table_id = 'mlops-363723.ChicagoTaxitrips.weather_hourly'\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_TRUNCATE\",  # overwrite existing table\n",
    ")\n",
    "\n",
    "try:\n",
    "    client.load_table_from_dataframe(df_hourly, table_id, job_config=job_config).result()\n",
    "    print(\"Data uploaded successfully to the main table.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during upload to the main table: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264a9894-0c69-4762-8293-6378cf300d5c",
   "metadata": {},
   "source": [
    "<span style='color:darkblue'><strong>Comment:</strong></span> <span style='color:black'>This code includes steps for preparing and uploading the `df_hourly` DataFrame to Google BigQuery. It starts by converting the index of `df_hourly` to datetime format and resetting it for compatibility. The data is first uploaded to a test table ('weather_hourly_test') to ensure integrity. If this upload is successful, indicated by the \"Data uploaded successfully\" message, the process proceeds to overwrite the main 'weather_hourly' table in BigQuery, using `WRITE_TRUNCATE` to replace existing data. Exception handling is implemented to catch and report any errors during the upload process.</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a65878-f4e6-4d18-99bc-a0791f5a5d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       time  temperature_2m  relativehumidity_2m  \\\n",
      "0 2020-09-04 17:00:00+00:00            25.4                   30   \n",
      "1 2020-09-04 16:00:00+00:00            24.9                   30   \n",
      "2 2017-03-08 13:00:00+00:00             8.6                   31   \n",
      "3 2015-10-17 17:00:00+00:00            10.8                   32   \n",
      "4 2016-03-02 13:00:00+00:00            -2.5                   32   \n",
      "5 2017-03-08 12:00:00+00:00             7.9                   32   \n",
      "6 2020-04-13 18:00:00+00:00             6.1                   32   \n",
      "7 2022-10-14 13:00:00+00:00            12.5                   32   \n",
      "8 2020-09-04 15:00:00+00:00            24.1                   32   \n",
      "9 2022-10-14 15:00:00+00:00            14.8                   32   \n",
      "\n",
      "   precipitation  rain  snowfall  weathercode  windspeed_10m  \n",
      "0            0.0   0.0       0.0            0           17.8  \n",
      "1            0.0   0.0       0.0            1           20.1  \n",
      "2            0.0   0.0       0.0            0           44.7  \n",
      "3            0.0   0.0       0.0            0           12.7  \n",
      "4            0.0   0.0       0.0            0           10.5  \n",
      "5            0.0   0.0       0.0            0           43.2  \n",
      "6            0.0   0.0       0.0            0           27.1  \n",
      "7            0.0   0.0       0.0            0           23.8  \n",
      "8            0.0   0.0       0.0            1           18.5  \n",
      "9            0.0   0.0       0.0            1           30.0  \n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client()\n",
    "query = \"\"\"\n",
    "SELECT * FROM `mlops-363723.ChicagoTaxitrips.weather_hourly` LIMIT 10;\n",
    "\"\"\"\n",
    "df = client.query(query).to_dataframe()\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177a24b6-1eca-465b-8013-62f70b09d0c9",
   "metadata": {},
   "source": [
    "<span style='color:darkblue'><strong>Comment:</strong></span> <span style='color:black'>This segment of code is dedicated to retrieving data from Google BigQuery. It starts by initializing a BigQuery client. Then, a query is defined to select the first 10 records from the 'weather_hourly' table. The query is executed, and the results are converted into a DataFrame named `df`, which is then printed. This process is essential for verifying the successful upload and integrity of the data in BigQuery, ensuring that the stored data is accessible and correctly formatted for analysis.</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c760c100-d506-426a-8ddb-2bdf720aaa3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: `mlops-363723.ChicagoTaxitrips.weather_hourly`\n",
      "Total Rows: 93744\n",
      "Missing Values: 0\n",
      "Missing Percentage: 0.00%\n",
      "\n",
      "Table: `mlops-363723.ChicagoTaxitrips.taxi_trips_demand`\n",
      "Total Rows: 14218579\n",
      "Missing Values: 0\n",
      "Missing Percentage: 0.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up the BigQuery client\n",
    "client = bigquery.Client(project='mlops-363723')\n",
    "\n",
    "# Define the table names\n",
    "weather_hourly_table = '`mlops-363723.ChicagoTaxitrips.weather_hourly`'\n",
    "taxi_trips_demand_table = '`mlops-363723.ChicagoTaxitrips.taxi_trips_demand`'\n",
    "\n",
    "# Function to calculate missing values and percentages\n",
    "def calculate_missing_values(table_name):\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        COUNT(*) AS total_rows,\n",
    "        COUNTIF({table_name} IS NULL) AS missing_values,\n",
    "        SAFE_DIVIDE(COUNTIF({table_name} IS NULL), COUNT(*)) * 100 AS missing_percentage\n",
    "    FROM\n",
    "        {table_name}\n",
    "    \"\"\"\n",
    "    query_job = client.query(query)\n",
    "    result = query_job.result()\n",
    "    for row in result:\n",
    "        total_rows = row.total_rows\n",
    "        missing_values = row.missing_values\n",
    "        missing_percentage = row.missing_percentage\n",
    "        print(f\"Table: {table_name}\")\n",
    "        print(f\"Total Rows: {total_rows}\")\n",
    "        print(f\"Missing Values: {missing_values}\")\n",
    "        print(f\"Missing Percentage: {missing_percentage:.2f}%\")\n",
    "        print()\n",
    "\n",
    "# Calculate missing values for weather_hourly table\n",
    "calculate_missing_values(weather_hourly_table)\n",
    "\n",
    "# Calculate missing values for taxi_trips_demand table\n",
    "calculate_missing_values(taxi_trips_demand_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2bdb87-10f8-477f-a5bb-73f9c5be2c29",
   "metadata": {},
   "source": [
    "<span style='color:darkblue'><strong>Comment:</strong></span> <span style='color:black'>This code establishes a procedure for assessing missing values in two key BigQuery tables: `weather_hourly` and `taxi_trips_demand`. After setting up the BigQuery client, a function `calculate_missing_values` is defined to compute and print the total number of rows, the count and percentage of missing values for each table. This function is then executed for both tables, providing vital insights into the completeness and integrity of our datasets. The results indicate the health of the data in terms of missing information, which is crucial for ensuring the reliability of subsequent analyses.</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ef1cf6-34ae-4ace-9a8a-9eb1430fa68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: unique_key\n",
      "Missing Count: 0\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 0.00%\n",
      "\n",
      "\n",
      "Column: taxi_id\n",
      "Missing Count: 0\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 0.00%\n",
      "\n",
      "\n",
      "Column: trip_start_timestamp\n",
      "Missing Count: 0\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 0.00%\n",
      "\n",
      "\n",
      "Column: trip_end_timestamp\n",
      "Missing Count: 0\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 0.00%\n",
      "\n",
      "\n",
      "Column: trip_seconds\n",
      "Missing Count: 13740\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 0.01%\n",
      "\n",
      "\n",
      "Column: trip_miles\n",
      "Missing Count: 2506\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 0.00%\n",
      "\n",
      "\n",
      "Column: pickup_census_tract\n",
      "Missing Count: 47760061\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 27.05%\n",
      "\n",
      "\n",
      "Column: dropoff_census_tract\n",
      "Missing Count: 47760061\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 27.05%\n",
      "\n",
      "\n",
      "Column: pickup_community_area\n",
      "Missing Count: 0\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 0.00%\n",
      "\n",
      "\n",
      "Column: dropoff_community_area\n",
      "Missing Count: 0\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 0.00%\n",
      "\n",
      "\n",
      "Column: fare\n",
      "Missing Count: 8887\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 0.01%\n",
      "\n",
      "\n",
      "Column: tips\n",
      "Missing Count: 8887\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 0.01%\n",
      "\n",
      "\n",
      "Column: tolls\n",
      "Missing Count: 29131740\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 16.50%\n",
      "\n",
      "\n",
      "Column: extras\n",
      "Missing Count: 8887\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 0.01%\n",
      "\n",
      "\n",
      "Column: trip_total\n",
      "Missing Count: 8887\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 0.01%\n",
      "\n",
      "\n",
      "Column: payment_type\n",
      "Missing Count: 0\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 0.00%\n",
      "\n",
      "\n",
      "Column: company\n",
      "Missing Count: 26128294\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 14.80%\n",
      "\n",
      "\n",
      "Column: pickup_latitude\n",
      "Missing Count: 0\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 0.00%\n",
      "\n",
      "\n",
      "Column: pickup_longitude\n",
      "Missing Count: 0\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 0.00%\n",
      "\n",
      "\n",
      "Column: pickup_location\n",
      "Missing Count: 0\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 0.00%\n",
      "\n",
      "\n",
      "Column: dropoff_latitude\n",
      "Missing Count: 0\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 0.00%\n",
      "\n",
      "\n",
      "Column: dropoff_longitude\n",
      "Missing Count: 0\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 0.00%\n",
      "\n",
      "\n",
      "Column: dropoff_location\n",
      "Missing Count: 0\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 0.00%\n",
      "\n",
      "\n",
      "Column: public_holiday\n",
      "Missing Count: 0\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 0.00%\n",
      "\n",
      "\n",
      "Column: rounded_timestamp\n",
      "Missing Count: 0\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 0.00%\n",
      "\n",
      "\n",
      "Column: time\n",
      "Missing Count: 0\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 0.00%\n",
      "\n",
      "\n",
      "Column: temperature_2m\n",
      "Missing Count: 0\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 0.00%\n",
      "\n",
      "\n",
      "Column: relativehumidity_2m\n",
      "Missing Count: 0\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 0.00%\n",
      "\n",
      "\n",
      "Column: precipitation\n",
      "Missing Count: 0\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 0.00%\n",
      "\n",
      "\n",
      "Column: rain\n",
      "Missing Count: 0\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 0.00%\n",
      "\n",
      "\n",
      "Column: snowfall\n",
      "Missing Count: 0\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 0.00%\n",
      "\n",
      "\n",
      "Column: weathercode\n",
      "Missing Count: 0\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 0.00%\n",
      "\n",
      "\n",
      "Column: shortwave_radiation\n",
      "Missing Count: 0\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 0.00%\n",
      "\n",
      "\n",
      "Column: windspeed_10m\n",
      "Missing Count: 0\n",
      "Total Count: 176539029\n",
      "Missing Percentage: 0.00%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the BigQuery client library\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Initialize a client for the 'mlops-363723' project\n",
    "client = bigquery.Client(project='mlops-363723')\n",
    "\n",
    "# Specify the table name\n",
    "table_name = '`mlops-363723.ChicagoTaxitrips.taxi_trips_demand_clean`'\n",
    "\n",
    "# Specify the schema of the table\n",
    "schema = ['unique_key', 'taxi_id', 'trip_start_timestamp', 'trip_end_timestamp',\n",
    "          'trip_seconds', 'trip_miles', 'pickup_census_tract', 'dropoff_census_tract', \n",
    "          'pickup_community_area', 'dropoff_community_area', 'fare', 'tips', 'tolls', \n",
    "          'extras', 'trip_total', 'payment_type', 'company', 'pickup_latitude',\n",
    "          'pickup_longitude', 'pickup_location', 'dropoff_latitude', \n",
    "          'dropoff_longitude', 'dropoff_location', 'public_holiday',\n",
    "          'rounded_timestamp', 'time', 'temperature_2m', 'relativehumidity_2m', \n",
    "          'precipitation', 'rain', 'snowfall', 'weathercode', 'shortwave_radiation', \n",
    "          'windspeed_10m']\n",
    "\n",
    "# For each column in the schema\n",
    "for column in schema:\n",
    "    # Create a SQL query\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        COUNTIF({column} IS NULL) AS missing_count,\n",
    "        COUNT(*) AS total_count,\n",
    "        (COUNTIF({column} IS NULL) / COUNT(*)) * 100 AS missing_percentage\n",
    "    FROM {table_name}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Run the query\n",
    "    query_job = client.query(query)\n",
    "    \n",
    "    # Fetch the results\n",
    "    result = query_job.result()\n",
    "    \n",
    "    # For each row in the results\n",
    "    for row in result:\n",
    "        # Print the column name, missing count, total count, and missing percentage\n",
    "        print(f\"Column: {column}\")\n",
    "        print(f\"Missing Count: {row.missing_count}\")\n",
    "        print(f\"Total Count: {row.total_count}\")\n",
    "        print(f\"Missing Percentage: {row.missing_percentage:.2f}%\")\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08d6142-d9db-4ea8-b015-b63bf684c0d5",
   "metadata": {},
   "source": [
    "<span style='color:darkblue'><strong>Comment:</strong></span> <span style='color:black'>In this segment, the code is designed to perform a detailed assessment of missing values for each column in the 'taxi_trips_demand_clean' table in BigQuery. The process begins with initializing a BigQuery client and specifying the table schema. For each column in the schema, a SQL query is constructed and executed to calculate the number and percentage of missing values. The results are printed, providing a column-wise breakdown of missing data. This thorough analysis is instrumental in identifying potential data quality issues and guiding data cleaning efforts, ensuring the robustness of the dataset for further analysis.</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e8a6ff-d750-46bf-8246-040e9b1e53b4",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3 style='color:darkblue; text-align:center;'><strong>Conclusion</strong></h3>\n",
    "<p style='color:black; font-size:16px; text-align:justify;'>\n",
    "In our comprehensive analysis, the decision to focus on <em style='color:darkgreen;'><strong>hourly data granularity</strong></em> was deliberate and crucial. This approach is grounded in the understanding that both weather conditions and taxi trip demand are highly dynamic, exhibiting significant variations within even a single day. By dissecting data on an hourly scale, particularly at the community area level, we unlock a more detailed and actionable insight into the interplay between weather patterns and taxi demand. This granularity is key in accurately capturing the ebb and flow of demand that unfolds over the course of the day, leading to enhanced and effective demand forecasting models. Ultimately, this strategy is tailored to yield granular, timely insights, vital for informed decision-making and efficient resource distribution in the ever-evolving landscape of taxi services in Chicago.\n",
    "</p>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26a729f-390f-41d7-8cc4-5df606307e45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
