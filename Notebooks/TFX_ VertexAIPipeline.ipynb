{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6169396-26d7-4535-ad78-40c7495306a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 05:20:11.155760: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-27 05:20:12.674999: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6Status12empty_stringB5cxx11Ev']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZNK10tensorflow4data11DatasetBase8FinalizeEPNS_15OpKernelContextESt8functionIFN3tsl8StatusOrISt10unique_ptrIS1_NS5_4core15RefCountDeleterEEEEvEE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n",
      "WARNING:root:TensorFlow Decision Forests 1.8.1 is compatible with the following TensorFlow Versions: ['2.15.0']. However, TensorFlow 2.13.1 was detected. This can cause issues with the TF API and symbols in the custom C++ ops. See the TF and TF-DF compatibility table at https://github.com/tensorflow/decision-forests/blob/main/documentation/known_issues.md#compatibility-table.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import shutil\n",
    "import tempfile\n",
    "import urllib.request\n",
    "from absl import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_data_validation as tfdv\n",
    "\n",
    "import tensorflow_model_analysis as tfma\n",
    "from tfx import v1 as tfx\n",
    "\n",
    "# Setting up logging and pprint\n",
    "# tf.get_logger().propagate = False\n",
    "pp = pprint.PrettyPrinter()\n",
    "# Set TensorFlow logging to error-only\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0 = all, 1 = no info, 2 = no info and warnings, 3 = no info, warnings, and errors\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3234dcd3-4797-4573-b4a1-99ee6df515eb",
   "metadata": {},
   "source": [
    "***Paths***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bf917cd-1a33-440e-8cb1-70cb55363e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_ROOT: gs://chicago_taxitrips/pipeline_root/demand-prediction\n",
      "DATA_DIRECTORY: gs://chicago_taxitrips/DATA_DIRECTORY\n",
      "TRAIN_DATA_PATH: gs://chicago_taxitrips/DATA_DIRECTORY/training_data.csv\n",
      "TEST_DATA_PATH: gs://chicago_taxitrips/DATA_DIRECTORY/test_data.csv\n",
      "VALIDATION_DATA_PATH: gs://chicago_taxitrips/DATA_DIRECTORY/validation_data.csv\n",
      "PIPELINE_ROOT: gs://chicago_taxitrips/pipeline_root/demand-prediction\n",
      "SERVING_MODEL_DIR: gs://chicago_taxitrips/serving_model/demand-prediction\n"
     ]
    }
   ],
   "source": [
    "GOOGLE_CLOUD_PROJECT = 'mlops-363723'         \n",
    "GOOGLE_CLOUD_PROJECT_NUMBER = '75674212269'  \n",
    "GOOGLE_CLOUD_REGION = 'us-central1'          \n",
    "GCS_BUCKET_NAME = 'chicago_taxitrips'  \n",
    "\n",
    "PIPELINE_NAME = 'demand-prediction'\n",
    "# Define the GCS_BUCKET_NAME\n",
    "GCS_BUCKET_NAME = 'chicago_taxitrips'\n",
    "DATA_DIRECTORY = 'gs://chicago_taxitrips/DATA_DIRECTORY'\n",
    "\n",
    "# # Paths for users' data.\n",
    "TRAIN_DATA_FILENAME = 'training_data.csv'\n",
    "VALIDATION_DATA_FILENAME = 'validation_data.csv'\n",
    "TEST_DATA_FILENAME = 'test_data.csv'\n",
    "\n",
    "TRAIN_DATA_PATH = os.path.join(DATA_DIRECTORY, TRAIN_DATA_FILENAME)\n",
    "VALIDATION_DATA_PATH = os.path.join(DATA_DIRECTORY, VALIDATION_DATA_FILENAME)\n",
    "TEST_DATA_PATH = os.path.join(DATA_DIRECTORY, TEST_DATA_FILENAME)\n",
    "\n",
    "\n",
    "# Path to various pipeline artifact.\n",
    "PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(\n",
    "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "# Paths for users' Python module.\n",
    "MODULE_ROOT = 'gs://{}/pipeline_module/{}'.format(\n",
    "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "\n",
    "# Path for TensorBoard logs.\n",
    "TENSORBOARD_LOG_DIR = 'gs://{}/tensorboard_logs/{}'.format(GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "# This is the path where your model will be pushed for serving.\n",
    "SERVING_MODEL_DIR = 'gs://{}/serving_model/{}'.format(\n",
    "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "# Name of Vertex AI Endpoint.\n",
    "ENDPOINT_NAME = 'prediction-' + PIPELINE_NAME\n",
    "\n",
    "print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))\n",
    "print('DATA_DIRECTORY:', DATA_DIRECTORY)\n",
    "print('TRAIN_DATA_PATH:', TRAIN_DATA_PATH)\n",
    "print('TEST_DATA_PATH:', TEST_DATA_PATH)\n",
    "print('VALIDATION_DATA_PATH:', VALIDATION_DATA_PATH)\n",
    "print('PIPELINE_ROOT:', PIPELINE_ROOT)\n",
    "print('SERVING_MODEL_DIR:', SERVING_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e6781dc-9c72-4ed6-836a-43a249cbbb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "_taxi_constants_module_file = 'taxi_constants.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b999b74a-8a83-495e-96f6-e2135e06b1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting taxi_constants.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_taxi_constants_module_file}\n",
    "NUMERICAL_FEATURES = [ 'trip_total',\n",
    "    'trip_miles',\n",
    "    'duration',\n",
    "    'temperature_2m',\n",
    "    'relativehumidity_2m',\n",
    "    'precipitation',\n",
    "    'rain',\n",
    "    'snowfall','hour_sin',\n",
    "    'hour_cos',\n",
    "    'day_sin',\n",
    "    'day_cos',\n",
    "    'month_sin',\n",
    "    'month_cos']\n",
    "\n",
    "\n",
    "\n",
    "CATEGORICAL_NUMERICAL_FEATURES = [\n",
    "    'public_holiday', 'weathercode','pickup_community_area', 'year'\n",
    "]\n",
    "\n",
    "\n",
    "# Keys\n",
    "LABEL_KEY = 'demand'\n",
    "\n",
    "def t_name(key):\n",
    "    \"\"\"\n",
    "    Rename the feature keys so that they don't clash with the raw keys when\n",
    "    running the Evaluator component.\n",
    "    Args:\n",
    "    key: The original feature key\n",
    "    Returns:\n",
    "    key with '_xf' appended\n",
    "    \"\"\"\n",
    "    return key + '_xf'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d7489d9-6b3a-45b4-b3a1-d6c2113ee07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORM_MODULE_PATH = \"chicago_taxi_transform.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3330792-7b00-43b3-a4e5-79336d25c18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting chicago_taxi_transform.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRANSFORM_MODULE_PATH}\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import taxi_constants\n",
    "\n",
    "_NUMERICAL_FEATURES = taxi_constants.NUMERICAL_FEATURES\n",
    "_CATEGORICAL_NUMERICAL_FEATURES = taxi_constants.CATEGORICAL_NUMERICAL_FEATURES\n",
    "_LABEL_KEY = taxi_constants.LABEL_KEY\n",
    "\n",
    "def _fill_in_missing(x):\n",
    "    \"\"\"Replace missing values in a SparseTensor.\"\"\"\n",
    "    default_value = '' if x.dtype == tf.string else 0\n",
    "    if not isinstance(x, tf.sparse.SparseTensor):\n",
    "        return x\n",
    "    return tf.squeeze(\n",
    "        tf.sparse.to_dense(\n",
    "            tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n",
    "            default_value),\n",
    "        axis=1)\n",
    "\n",
    "def z_score_normalization(inputs, feature_name):\n",
    "    \"\"\"Apply Z-score normalization on a feature.\"\"\"\n",
    "    return tft.scale_to_z_score(_fill_in_missing(inputs[feature_name]))\n",
    "\n",
    "# def log_transformation(inputs, feature_name):\n",
    "#     \"\"\"Apply log transformation on a feature.\"\"\"\n",
    "#     feature_values = tf.where(inputs[feature_name] <= 0, tf.constant(0.01, dtype=tf.float32), inputs[feature_name])\n",
    "#     return tf.math.log1p(feature_values)\n",
    "def log_transformation(inputs, feature_name):\n",
    "    \"\"\"Apply log transformation on a feature.\"\"\"\n",
    "    # Ensure the input feature is in float32\n",
    "    feature_values = tf.cast(inputs[feature_name], tf.float32)\n",
    "\n",
    "    # Replace non-positive values with a small positive number\n",
    "    feature_values_safe = tf.where(\n",
    "        feature_values <= 0, \n",
    "        tf.constant(0.01, dtype=tf.float32),\n",
    "        feature_values\n",
    "    )\n",
    "\n",
    "    return tf.math.log1p(feature_values_safe)\n",
    "\n",
    "def square_root_transformation(inputs, feature_name):\n",
    "    \"\"\"Apply square root transformation on a feature.\"\"\"\n",
    "    feature_values = tf.where(inputs[feature_name] <= 0, tf.constant(0.01, dtype=tf.float32), inputs[feature_name])\n",
    "    return tf.math.sqrt(feature_values)\n",
    "\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"Preprocess input columns into transformed columns.\"\"\"\n",
    "    outputs = {}\n",
    "    \n",
    "    # Numerical Features: Apply Z-score normalization.\n",
    "    for key in _NUMERICAL_FEATURES:\n",
    "        outputs[taxi_constants.t_name(key)] = z_score_normalization(inputs, key)\n",
    "    \n",
    "    # Categorical Numerical Features\n",
    "    for key in _CATEGORICAL_NUMERICAL_FEATURES:\n",
    "        # Ensure the feature exists in the inputs\n",
    "        if key in inputs:\n",
    "            outputs[taxi_constants.t_name(key)] = _fill_in_missing(inputs[key])\n",
    "\n",
    "    # Feature Transformations\n",
    "    outputs['log_trip_total'] = log_transformation(inputs, 'trip_total')\n",
    "    outputs['log_trip_miles'] = log_transformation(inputs, 'trip_miles')\n",
    "    outputs['log_duration'] = log_transformation(inputs, 'duration')\n",
    "    outputs['sqrt_precipitation'] = square_root_transformation(inputs, 'precipitation')\n",
    "    \n",
    "    if _LABEL_KEY in inputs:\n",
    "        \n",
    "        outputs[_LABEL_KEY]= log_transformation(inputs, _LABEL_KEY)\n",
    "        \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76572a50-eda6-4e4b-a8d3-4d1071018be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_taxi_trainer_module_file = 'taxi_trainer.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec9e4465-2549-457c-a86c-b5ae9fd4b656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting taxi_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_taxi_trainer_module_file}\n",
    "\n",
    "import tensorflow as tf\n",
    "from tfx import v1 as tfx\n",
    "from tfx.components.trainer.fn_args_utils import FnArgs\n",
    "import os\n",
    "from typing import NamedTuple, Dict, List, Text, Any\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras_tuner.engine import base_tuner\n",
    "from tfx_bsl.public import tfxio\n",
    "from tfx.components.trainer.fn_args_utils import FnArgs\n",
    "import os\n",
    "import taxi_constants\n",
    "\n",
    "_LABEL_KEY = taxi_constants.LABEL_KEY\n",
    "_BATCH_SIZE = 40\n",
    "early_stopping = EarlyStopping(monitor='val_mean_absolute_error', patience=3, restore_best_weights=True)\n",
    "\n",
    "def _input_fn(file_pattern: List[Text], data_accessor: tfx.components.DataAccessor, tf_transform_output: tft.TFTransformOutput, batch_size: int = _BATCH_SIZE) -> tf.data.Dataset:\n",
    "    return data_accessor.tf_dataset_factory(file_pattern, tfxio.TensorFlowDatasetOptions(batch_size=batch_size, label_key=_LABEL_KEY), tf_transform_output.transformed_metadata.schema).repeat()\n",
    "\n",
    "def _build_keras_model(tf_transform_output: tft.TFTransformOutput) -> tf.keras.Model:\n",
    "    feature_spec = tf_transform_output.transformed_feature_spec().copy()\n",
    "    feature_spec.pop(_LABEL_KEY)\n",
    "\n",
    "    inputs = []\n",
    "    processed_inputs = []\n",
    "    for key, spec in feature_spec.items():\n",
    "        if isinstance(spec, tf.io.VarLenFeature):\n",
    "            inputs.append(tf.keras.layers.Input(shape=[None], name=key, dtype=spec.dtype, sparse=True))\n",
    "            processed_inputs.append(tf.keras.layers.Flatten()(tf.keras.layers.DenseFeatures(spec)(inputs[-1])))\n",
    "        elif isinstance(spec, tf.io.FixedLenFeature):\n",
    "            inputs.append(tf.keras.layers.Input(shape=spec.shape, name=key, dtype=spec.dtype))\n",
    "            processed_inputs.append(inputs[-1])\n",
    "\n",
    "    concatenated_inputs = tf.keras.layers.Concatenate()(processed_inputs)\n",
    "\n",
    "    # Hardcoded hyperparameters\n",
    "    units_0 = 120\n",
    "    dropout_0 = False\n",
    "    learning_rate = 0.0008\n",
    "    dropout_rate_0 = 0.13\n",
    "\n",
    "    # Build the model\n",
    "    concatenated_inputs = tf.keras.layers.Dense(units=units_0, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(concatenated_inputs)\n",
    "    if dropout_0:\n",
    "        concatenated_inputs = tf.keras.layers.Dropout(dropout_rate_0)(concatenated_inputs)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(1)(concatenated_inputs)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=[tf.keras.metrics.MeanAbsoluteError(), tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "    return model\n",
    "def _get_tf_examples_serving_signature(model, tf_transform_output):\n",
    "    model.tft_layer_inference = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n",
    "    ])\n",
    "    def serve_tf_examples_fn(serialized_tf_example):\n",
    "        raw_feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        raw_feature_spec.pop(_LABEL_KEY)\n",
    "        raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n",
    "        transformed_features = model.tft_layer_inference(raw_features)\n",
    "        outputs = model(transformed_features)\n",
    "        return {'outputs': outputs}\n",
    "\n",
    "    return serve_tf_examples_fn\n",
    "\n",
    "def _get_transform_features_signature(model, tf_transform_output):\n",
    "    model.tft_layer_eval = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n",
    "    ])\n",
    "    def transform_features_fn(serialized_tf_example):\n",
    "        raw_feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n",
    "        transformed_features = model.tft_layer_eval(raw_features)\n",
    "        return transformed_features\n",
    "\n",
    "    return transform_features_fn\n",
    "def run_fn(fn_args: FnArgs):\n",
    "    # Setup paths for TensorBoard logs\n",
    "    working_dir = fn_args.working_dir or '/tmp'\n",
    "    log_dir = os.path.join(working_dir, 'logs')\n",
    "    model_dir = os.path.join(working_dir, 'model')\n",
    "\n",
    "    # Model Checkpoint Callback\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=os.path.join(model_dir, 'best_model'),\n",
    "        monitor='val_mean_absolute_error',\n",
    "        save_best_only=True\n",
    "    )\n",
    "\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
    "\n",
    "    # Build the Keras model (with hardcoded hyperparameters)\n",
    "    model = _build_keras_model(tf_transform_output)\n",
    "\n",
    "    # Prepare the datasets\n",
    "    train_dataset = _input_fn(\n",
    "        fn_args.train_files,\n",
    "        fn_args.data_accessor,\n",
    "        tf_transform_output,\n",
    "        _BATCH_SIZE\n",
    "    )\n",
    "    eval_dataset = _input_fn(\n",
    "        fn_args.eval_files,\n",
    "        fn_args.data_accessor,\n",
    "        tf_transform_output,\n",
    "        _BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    # Fit the model\n",
    "    total_train_examples = fn_args.train_steps * _BATCH_SIZE\n",
    "    steps_per_epoch = total_train_examples // _BATCH_SIZE\n",
    "    epochs = fn_args.train_steps // steps_per_epoch\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=eval_dataset,\n",
    "        validation_steps=fn_args.eval_steps,\n",
    "        epochs=epochs,\n",
    "        callbacks=[checkpoint_callback, tensorboard_callback]\n",
    "    )\n",
    "\n",
    "    # Save the model\n",
    "    export_serving_model(tf_transform_output, model, fn_args.serving_model_dir)\n",
    "\n",
    "\n",
    "def export_serving_model(tf_transform_output, model, output_dir):\n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "    signatures = {\n",
    "        'serving_default': _get_tf_examples_serving_signature(model, tf_transform_output),\n",
    "        'transform_features': _get_transform_features_signature(model, tf_transform_output),\n",
    "    }\n",
    "    model.save(output_dir, save_format='tf', signatures=signatures)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9492d4a1-ba21-4cd8-b35a-315535bd0c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {_taxi_trainer_module_file}\n",
    "from typing import NamedTuple, Dict, List, Text, Any\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tfx import v1 as tfx\n",
    "from keras_tuner.engine import base_tuner\n",
    "from keras_tuner import HyperParameters, RandomSearch,BayesianOptimization\n",
    "from tfx_bsl.public import tfxio\n",
    "from tfx.components.trainer.fn_args_utils import FnArgs\n",
    "import os\n",
    "import taxi_constants\n",
    "_LABEL_KEY = taxi_constants.LABEL_KEY\n",
    "\n",
    "\n",
    "_BATCH_SIZE = 64\n",
    "\n",
    "TunerFnResult = NamedTuple('TunerFnResult', [('tuner', BayesianOptimization), ('fit_kwargs', Dict[Text, Any])])  # Changed to BayesianOptimization\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_mean_absolute_error',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "def _input_fn(file_pattern: List[Text],\n",
    "              data_accessor: tfx.components.DataAccessor,\n",
    "              tf_transform_output: tft.TFTransformOutput,\n",
    "              batch_size: int = _BATCH_SIZE) -> tf.data.Dataset:\n",
    "    return data_accessor.tf_dataset_factory(\n",
    "        file_pattern,\n",
    "        tfxio.TensorFlowDatasetOptions(batch_size=batch_size, label_key=_LABEL_KEY),\n",
    "        tf_transform_output.transformed_metadata.schema).repeat()\n",
    "\n",
    "\n",
    "def _build_keras_model(hp, tf_transform_output: tft.TFTransformOutput) -> tf.keras.Model:\n",
    "    # Define feature specs and create input layers\n",
    "    feature_spec = tf_transform_output.transformed_feature_spec().copy()\n",
    "    # Remove the label feature\n",
    "    feature_spec.pop(_LABEL_KEY)\n",
    "    inputs = {\n",
    "        key: tf.keras.layers.Input(shape=(1,), name=key)\n",
    "        for key in feature_spec.keys()\n",
    "    }\n",
    "\n",
    "    # Concatenate all input features\n",
    "    concatenated_inputs = tf.keras.layers.Concatenate()(list(inputs.values()))\n",
    "\n",
    "    num_layers = hp.Int('num_layers', 1, 5)\n",
    "    activation_choice = hp.Choice('activation', ['relu', 'leaky_relu', 'elu', 'tanh', 'sigmoid'])\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        units = hp.Int(f'units_{i}', min_value=32, max_value=512, step=32)\n",
    "        concatenated_inputs = tf.keras.layers.Dense(\n",
    "            units=units,\n",
    "            activation=activation_choice,\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(hp.Float('l2_{i}', 1e-5, 1e-2, sampling='log'))\n",
    "        )(concatenated_inputs)\n",
    "        if hp.Boolean(f'dropout_{i}'):\n",
    "            dropout_rate = hp.Float(f'dropout_rate_{i}', 0.1, 0.5)\n",
    "            concatenated_inputs = tf.keras.layers.Dropout(dropout_rate)(concatenated_inputs)\n",
    "\n",
    "    # Output layer for regression\n",
    "    output = tf.keras.layers.Dense(1, activation='linear')(concatenated_inputs)\n",
    "\n",
    "    # Create and compile the Keras model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "        loss='mean_squared_error',\n",
    "        metrics=[\n",
    "            tf.keras.metrics.MeanAbsoluteError(),\n",
    "            tf.keras.metrics.RootMeanSquaredError()\n",
    "        ]\n",
    "    )\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def tuner_fn(fn_args: FnArgs) -> TunerFnResult:\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
    "    tuner = BayesianOptimization(\n",
    "        hypermodel=lambda hp: _build_keras_model(hp, tf_transform_output),\n",
    "        objective='val_mean_absolute_error',\n",
    "        max_trials=25,\n",
    "        executions_per_trial=1,\n",
    "        directory=fn_args.working_dir,\n",
    "        project_name='taxi_trips_tuning_bayesian_optimization'\n",
    "    )\n",
    "\n",
    "    train_dataset = _input_fn(fn_args.train_files, fn_args.data_accessor, tf_transform_output, _BATCH_SIZE)\n",
    "    eval_dataset = _input_fn(fn_args.eval_files, fn_args.data_accessor, tf_transform_output, _BATCH_SIZE)\n",
    "\n",
    "    return TunerFnResult(\n",
    "        tuner=tuner,\n",
    "        fit_kwargs={\n",
    "            'x': train_dataset,\n",
    "            'validation_data': eval_dataset,\n",
    "            'steps_per_epoch': 1500,  \n",
    "            'validation_steps': 969,  \n",
    "            'callbacks': [early_stopping]\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def _get_tf_examples_serving_signature(model, tf_transform_output):\n",
    "    model.tft_layer_inference = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n",
    "    ])\n",
    "    def serve_tf_examples_fn(serialized_tf_example):\n",
    "        raw_feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n",
    "        transformed_features = model.tft_layer_inference(raw_features)\n",
    "        model_input_keys = [layer.name for layer in model.layers if isinstance(layer, tf.keras.layers.InputLayer)]\n",
    "        filtered_features = {key: value for key, value in transformed_features.items() if key in model_input_keys}\n",
    "        outputs = model(filtered_features)\n",
    "        return {'outputs': outputs}\n",
    "\n",
    "    return serve_tf_examples_fn\n",
    "\n",
    "\n",
    "\n",
    "def _get_transform_features_signature(model, tf_transform_output):\n",
    "    model.tft_layer_eval = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n",
    "    ])\n",
    "    def transform_features_fn(serialized_tf_example):\n",
    "        raw_feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n",
    "        transformed_features = model.tft_layer_eval(raw_features)\n",
    "        return transformed_features\n",
    "\n",
    "    return transform_features_fn\n",
    "\n",
    "def export_serving_model(tf_transform_output, model, output_dir):\n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "    signatures = {\n",
    "        'serving_default': _get_tf_examples_serving_signature(model, tf_transform_output),\n",
    "        'transform_features': _get_transform_features_signature(model, tf_transform_output),\n",
    "    }\n",
    "    model.save(output_dir, save_format='tf', signatures=signatures)\n",
    "    \n",
    "def run_fn(fn_args: FnArgs):\n",
    "    tensorboard_log_dir = os.path.join(fn_args.model_run_dir, 'logs')\n",
    "    model_dir = os.path.join(fn_args.model_run_dir, 'model')\n",
    "    os.makedirs(tensorboard_log_dir, exist_ok=True)\n",
    "\n",
    "  \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(fn_args.model_run_dir, 'logs'),  \n",
    "        update_freq='batch'\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_mean_absolute_error', \n",
    "        mode='min', \n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    \n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(model_dir, 'best_model'),\n",
    "        monitor='val_mean_absolute_error', \n",
    "        mode='min', \n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    " \n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
    "\n",
    "    # Set up the tuner for hyperparameter tuning\n",
    "    tuner_fn_result = tuner_fn(fn_args)\n",
    "    tuner = tuner_fn_result.tuner\n",
    "\n",
    "    # Create datasets for training and evaluation\n",
    "    train_dataset = _input_fn(\n",
    "        fn_args.train_files, \n",
    "        fn_args.data_accessor, \n",
    "        tf_transform_output, \n",
    "        _BATCH_SIZE\n",
    "    )\n",
    "    eval_dataset = _input_fn(\n",
    "        fn_args.eval_files, \n",
    "        fn_args.data_accessor, \n",
    "        tf_transform_output, \n",
    "        _BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    # Perform hyperparameter search using the tuner\n",
    "    tuner.search(**tuner_fn_result.fit_kwargs)\n",
    "\n",
    "    # Retrieve the best hyperparameters\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    # Build the best Keras model based on the best hyperparameters\n",
    "    model = _build_keras_model(best_hps, tf_transform_output)\n",
    "\n",
    "    # Determine the number of steps per epoch\n",
    "    total_train_examples = fn_args.train_steps * _BATCH_SIZE\n",
    "    steps_per_epoch = total_train_examples // _BATCH_SIZE\n",
    "\n",
    "    # Fit the model with the callbacks\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=eval_dataset,\n",
    "        validation_steps=fn_args.eval_steps,\n",
    "        epochs=fn_args.train_steps // steps_per_epoch,\n",
    "        callbacks=[\n",
    "            tensorboard_callback,\n",
    "            early_stopping_callback,\n",
    "            model_checkpoint_callback\n",
    "        ]\n",
    "    )\n",
    "    signatures = {\n",
    "      'serving_default': _get_tf_examples_serving_signature(model, tf_transform_output),\n",
    "    }\n",
    "    \n",
    "    # model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)\n",
    "    # Export the serving model\n",
    "    export_serving_model(tf_transform_output, model, fn_args.serving_model_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57335c5e-2942-442e-a54e-4663c6771f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://taxi_trainer.py [Content-Type=text/x-python]...\n",
      "/ [1 files][  5.7 KiB/  5.7 KiB]                                                \n",
      "Operation completed over 1 objects/5.7 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp {_taxi_trainer_module_file} {MODULE_ROOT}/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4531e8f2-6d1b-4d48-9362-a9687576c76b",
   "metadata": {},
   "source": [
    "***Pipeline Definition***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f96ac3c0-bc86-4674-bf53-5a1577e1c25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow_model_analysis as tfma\n",
    "import tfx\n",
    "from tfx import v1 as tfx\n",
    "from tfx.components import CsvExampleGen, StatisticsGen, SchemaGen, ExampleValidator, Transform, Trainer, Pusher,Evaluator\n",
    "from tfx.orchestration import pipeline\n",
    "from tfx.proto import example_gen_pb2, trainer_pb2, pusher_pb2\n",
    "import taxi_constants\n",
    "\n",
    "\n",
    "from tfx.proto import example_gen_pb2, pusher_pb2, trainer_pb2\n",
    "\n",
    "def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
    "                     module_file: str, endpoint_name: str, project_id: str,\n",
    "                     region: str, serving_model_dir: str) -> tfx.dsl.Pipeline:\n",
    "    # Define input data\n",
    "    input_config = example_gen_pb2.Input(splits=[\n",
    "        example_gen_pb2.Input.Split(name='train', pattern=TRAIN_DATA_FILENAME),\n",
    "        example_gen_pb2.Input.Split(name='validation', pattern=VALIDATION_DATA_FILENAME)\n",
    "    ])\n",
    "    example_gen = CsvExampleGen(input_base=data_root, input_config=input_config)\n",
    "\n",
    "    # Components for data analysis and validation\n",
    "    statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])\n",
    "    schema_gen = SchemaGen(statistics=statistics_gen.outputs['statistics'])\n",
    "    example_validator = ExampleValidator(\n",
    "        statistics=statistics_gen.outputs['statistics'],\n",
    "        schema=schema_gen.outputs['schema']\n",
    "    )\n",
    "\n",
    "    # Component for data transformation\n",
    "    transform = Transform(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        module_file=os.path.abspath(TRANSFORM_MODULE_PATH)\n",
    "\n",
    "    )\n",
    "    \n",
    "     # Configuration for Vertex AI Training\n",
    "    vertex_job_spec = {\n",
    "        'project': project_id,\n",
    "        'worker_pool_specs': [{\n",
    "            'machine_spec': {\n",
    "                'machine_type': 'n1-standard-4'\n",
    "            },\n",
    "            'replica_count': 1,\n",
    "            'container_spec': {\n",
    "                'image_uri': 'gcr.io/tfx-oss-public/tfx:{}'.format(tfx.__version__),\n",
    "            },\n",
    "        }],\n",
    "    }\n",
    "    \n",
    "    trainer = tfx.extensions.google_cloud_ai_platform.Trainer(\n",
    "        module_file=os.path.abspath(module_file),\n",
    "        transformed_examples=transform.outputs['transformed_examples'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        transform_graph=transform.outputs['transform_graph'],\n",
    "        train_args=tfx.proto.TrainArgs(splits=['train'], num_steps=100),\n",
    "        eval_args=tfx.proto.EvalArgs(splits=['validation'], num_steps=5),\n",
    "        custom_config={\n",
    "          tfx.extensions.google_cloud_ai_platform.ENABLE_VERTEX_KEY:\n",
    "              True,\n",
    "          tfx.extensions.google_cloud_ai_platform.VERTEX_REGION_KEY:\n",
    "              region,\n",
    "          tfx.extensions.google_cloud_ai_platform.TRAINING_ARGS_KEY:\n",
    "              vertex_job_spec,\n",
    "         \n",
    "      })\n",
    "    \n",
    "    vertex_serving_spec = {\n",
    "      'project_id': project_id,\n",
    "      'endpoint_name': endpoint_name,\n",
    "      'machine_type': 'n1-standard-4',\n",
    "  }\n",
    "    serving_image = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-12:latest'\n",
    "   \n",
    "\n",
    "    \n",
    "    eval_config = tfma.EvalConfig(\n",
    "    model_specs=[tfma.ModelSpec(signature_name='serving_default', \n",
    "                                label_key=taxi_constants.LABEL_KEY)],\n",
    "    slicing_specs=[tfma.SlicingSpec()],\n",
    "    metrics_specs=[\n",
    "        tfma.MetricsSpec(metrics=[\n",
    "            tfma.MetricConfig(class_name='MeanAbsoluteError'),\n",
    "            tfma.MetricConfig(class_name='RootMeanSquaredError')\n",
    "        ])\n",
    "    ])\n",
    "    \n",
    "    evaluator = Evaluator(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    example_splits=['validation'],  # Specify the split name here\n",
    "    model=trainer.outputs['model'],\n",
    "    # baseline_model=None,  # No baseline model for comparison\n",
    "    eval_config=eval_config\n",
    "    )\n",
    "   \n",
    "\n",
    "    pusher = tfx.extensions.google_cloud_ai_platform.Pusher(\n",
    "       model=trainer.outputs['model'],\n",
    "       model_blessing=evaluator.outputs['blessing'],  # Add model_blessing input\n",
    "       custom_config={\n",
    "          tfx.extensions.google_cloud_ai_platform.ENABLE_VERTEX_KEY:\n",
    "              True,\n",
    "          tfx.extensions.google_cloud_ai_platform.VERTEX_REGION_KEY:\n",
    "              region,\n",
    "          tfx.extensions.google_cloud_ai_platform.VERTEX_CONTAINER_IMAGE_URI_KEY:\n",
    "              serving_image,\n",
    "          tfx.extensions.google_cloud_ai_platform.SERVING_ARGS_KEY:\n",
    "            vertex_serving_spec,\n",
    "        })\n",
    "\n",
    "    # Define the pipeline\n",
    "    components = [\n",
    "        example_gen,\n",
    "        statistics_gen,\n",
    "        schema_gen,\n",
    "        example_validator,\n",
    "        transform,  \n",
    "        trainer,\n",
    "        evaluator,\n",
    "        pusher\n",
    "    ]\n",
    "\n",
    "    return tfx.dsl.Pipeline(\n",
    "        pipeline_name=pipeline_name,\n",
    "        pipeline_root=pipeline_root,\n",
    "        components=components,\n",
    "        \n",
    "       \n",
    "    )\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4db35d21-aad6-466a-a5d8-fa6c9670ea0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7335573c-fb2e-41b3-b6bb-9bb757996298",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`transformed_examples` is deprecated. Please use `examples` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying chicago_taxi_transform.py -> build/lib\n",
      "copying taxi_transform.py -> build/lib\n",
      "copying taxi_trainer.py -> build/lib\n",
      "copying taxi_constants.py -> build/lib\n",
      "copying taxi_tuner.py -> build/lib\n",
      "installing to /tmp/tmpxgth9e8h\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/taxi_transform.py -> /tmp/tmpxgth9e8h\n",
      "copying build/lib/taxi_tuner.py -> /tmp/tmpxgth9e8h\n",
      "copying build/lib/taxi_constants.py -> /tmp/tmpxgth9e8h\n",
      "copying build/lib/taxi_trainer.py -> /tmp/tmpxgth9e8h\n",
      "copying build/lib/chicago_taxi_transform.py -> /tmp/tmpxgth9e8h\n",
      "running install_egg_info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` directly.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running egg_info\n",
      "creating tfx_user_code_Transform.egg-info\n",
      "writing tfx_user_code_Transform.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_Transform.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_Transform.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_Transform.egg-info to /tmp/tmpxgth9e8h/tfx_user_code_Transform-0.0+721899f7d92e082e5e9c1e01a1c3e671bc0c4c3368738c0aa04597bd0e686b26-py3.10.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmpxgth9e8h/tfx_user_code_Transform-0.0+721899f7d92e082e5e9c1e01a1c3e671bc0c4c3368738c0aa04597bd0e686b26.dist-info/WHEEL\n",
      "creating '/tmp/tmpvi0vrql9/tfx_user_code_Transform-0.0+721899f7d92e082e5e9c1e01a1c3e671bc0c4c3368738c0aa04597bd0e686b26-py3-none-any.whl' and adding '/tmp/tmpxgth9e8h' to it\n",
      "adding 'chicago_taxi_transform.py'\n",
      "adding 'taxi_constants.py'\n",
      "adding 'taxi_trainer.py'\n",
      "adding 'taxi_transform.py'\n",
      "adding 'taxi_tuner.py'\n",
      "adding 'tfx_user_code_Transform-0.0+721899f7d92e082e5e9c1e01a1c3e671bc0c4c3368738c0aa04597bd0e686b26.dist-info/METADATA'\n",
      "adding 'tfx_user_code_Transform-0.0+721899f7d92e082e5e9c1e01a1c3e671bc0c4c3368738c0aa04597bd0e686b26.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_Transform-0.0+721899f7d92e082e5e9c1e01a1c3e671bc0c4c3368738c0aa04597bd0e686b26.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_Transform-0.0+721899f7d92e082e5e9c1e01a1c3e671bc0c4c3368738c0aa04597bd0e686b26.dist-info/RECORD'\n",
      "removing /tmp/tmpxgth9e8h\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying chicago_taxi_transform.py -> build/lib\n",
      "copying taxi_transform.py -> build/lib\n",
      "copying taxi_trainer.py -> build/lib\n",
      "copying taxi_constants.py -> build/lib\n",
      "copying taxi_tuner.py -> build/lib\n",
      "installing to /tmp/tmp98grnqe6\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/taxi_transform.py -> /tmp/tmp98grnqe6\n",
      "copying build/lib/taxi_tuner.py -> /tmp/tmp98grnqe6\n",
      "copying build/lib/taxi_constants.py -> /tmp/tmp98grnqe6\n",
      "copying build/lib/taxi_trainer.py -> /tmp/tmp98grnqe6\n",
      "copying build/lib/chicago_taxi_transform.py -> /tmp/tmp98grnqe6\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_Trainer.egg-info\n",
      "writing tfx_user_code_Trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_Trainer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_Trainer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` directly.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_Trainer.egg-info to /tmp/tmp98grnqe6/tfx_user_code_Trainer-0.0+721899f7d92e082e5e9c1e01a1c3e671bc0c4c3368738c0aa04597bd0e686b26-py3.10.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmp98grnqe6/tfx_user_code_Trainer-0.0+721899f7d92e082e5e9c1e01a1c3e671bc0c4c3368738c0aa04597bd0e686b26.dist-info/WHEEL\n",
      "creating '/tmp/tmp9uo6zhs8/tfx_user_code_Trainer-0.0+721899f7d92e082e5e9c1e01a1c3e671bc0c4c3368738c0aa04597bd0e686b26-py3-none-any.whl' and adding '/tmp/tmp98grnqe6' to it\n",
      "adding 'chicago_taxi_transform.py'\n",
      "adding 'taxi_constants.py'\n",
      "adding 'taxi_trainer.py'\n",
      "adding 'taxi_transform.py'\n",
      "adding 'taxi_tuner.py'\n",
      "adding 'tfx_user_code_Trainer-0.0+721899f7d92e082e5e9c1e01a1c3e671bc0c4c3368738c0aa04597bd0e686b26.dist-info/METADATA'\n",
      "adding 'tfx_user_code_Trainer-0.0+721899f7d92e082e5e9c1e01a1c3e671bc0c4c3368738c0aa04597bd0e686b26.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_Trainer-0.0+721899f7d92e082e5e9c1e01a1c3e671bc0c4c3368738c0aa04597bd0e686b26.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_Trainer-0.0+721899f7d92e082e5e9c1e01a1c3e671bc0c4c3368738c0aa04597bd0e686b26.dist-info/RECORD'\n",
      "removing /tmp/tmp98grnqe6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PIPELINE_DEFINITION_FILE = PIPELINE_NAME + '_pipeline.json'\n",
    "\n",
    "runner = tfx.orchestration.experimental.KubeflowV2DagRunner(\n",
    "    config=tfx.orchestration.experimental.KubeflowV2DagRunnerConfig(),\n",
    "    output_filename=PIPELINE_DEFINITION_FILE)\n",
    "# Following function will write the pipeline definition to PIPELINE_DEFINITION_FILE.\n",
    "_ = runner.run(\n",
    "    _create_pipeline(\n",
    "        pipeline_name=PIPELINE_NAME,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        data_root=DATA_DIRECTORY,\n",
    "        module_file=_taxi_trainer_module_file,\n",
    "        endpoint_name=ENDPOINT_NAME,\n",
    "        project_id=GOOGLE_CLOUD_PROJECT,\n",
    "        region=GOOGLE_CLOUD_REGION,\n",
    "        serving_model_dir=SERVING_MODEL_DIR,\n",
    "        \n",
    "       \n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa18c984-d8b5-4f09-ba8b-574a83d2682c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/75674212269/locations/us-central1/pipelineJobs/demand-prediction-20231127052108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/75674212269/locations/us-central1/pipelineJobs/demand-prediction-20231127052108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_job = aiplatform.PipelineJob.get('projects/75674212269/locations/us-central1/pipelineJobs/demand-prediction-20231127052108')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/75674212269/locations/us-central1/pipelineJobs/demand-prediction-20231127052108')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/demand-prediction-20231127052108?project=75674212269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/demand-prediction-20231127052108?project=75674212269\n"
     ]
    }
   ],
   "source": [
    "# docs_infra: no_execute\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "aiplatform.init(project=GOOGLE_CLOUD_PROJECT, location=GOOGLE_CLOUD_REGION)\n",
    "\n",
    "job = pipeline_jobs.PipelineJob(template_path=PIPELINE_DEFINITION_FILE,\n",
    "                                display_name=PIPELINE_NAME)\n",
    "job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a44753-9965-492f-ad41-59cf978a5c14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2.12 (Local)",
   "language": "python",
   "name": "local-tf2-2-12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
